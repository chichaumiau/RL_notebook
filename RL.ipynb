{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:7c6f12e824583211f1674f1f74d65e6aae47e87fde2f18f9a1697a14b804f245"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work with is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S \\times A$, $R(s,a)$ is a function that maps states/actions to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ after taking $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n",
      "    \n",
      "* This lab is just a demo of some of the principles we talked in class, now with transition loops\n",
      "* Most of the code is based on the book Artificial Intelligence: A Modern Approach \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615,AIM]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.99):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state, action):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state][action]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "        \n",
      "    def sample(state, action):\n",
      "        \"\"\"Sample from state action. Returns a new state\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Grid MDP\n",
      "===\n",
      "* We can now define a helper class called GridMDP, to help us encode a grid-like MDP\n",
      "* We will only use deterministic transitions\n",
      "    * ... but there are loops!\n",
      "* We will not use terminal states in our examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1, AIM].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.rows):\n",
      "            for y in range(self.cols):\n",
      "                self.reward[x, y] = grid[x][y]\n",
      "                if grid[x][y] is not None:\n",
      "                    self.states.add((x, y))\n",
      "        self.orig_grid = grid\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return 0.0, state\n",
      "        else:\n",
      "            return 1.0, self.go(state, action)\n",
      "        \n",
      "    def R(self, state, action):\n",
      "        \"Return a numeric reward for this state/action pair.\"\n",
      "        p_state_next, state_next = self.T(state,action)\n",
      "        if(state_next == state):\n",
      "            return 0\n",
      "        return self.reward[state_next]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating a sample GridMDP\n",
      "===\n",
      "* We can create now a simple MDP just by defining an 2D array structure\n",
      "* The values of the grid are the rewards at each (x,y) position"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "grid = [[-0.00, -0.00, -0.00, +1],\n",
      "        [-0.00, -0.00, -0.00, -1],\n",
      "        [-0.00, -0.00, -0.00, -0.00]]\n",
      "terminals = [(0, 3), (1, 3)]\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt(grid, fig = \"grid.png\")\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named utils",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-5ba8116e5550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m grid = [[-0.00, -0.00, -0.00, +1],\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         [-0.00, -0.00, -0.00, -0.00]]\n\u001b[1;32m      5\u001b[0m \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named utils"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q-Learning\n",
      "=== \n",
      "* We can now define a class for Q-learning agents\n",
      "* Notice how we have a deterministic option so the agent can choose how to update q-values\n",
      "* \"Real\" Q-learning not explicitly needed in our simple scenario, but interesting option"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "class QAgent():\n",
      "    \"\"\" A Q-learning iteration agent\n",
      "    \"\"\"\n",
      "    def __init__(self,mdp, k):\n",
      "        self.mdp = mdp\n",
      "        self.k = k\n",
      "    \n",
      "\n",
      "    def QLearning(self, iterations, initial_state, deterministic = True, learning_rate = 0.01):\n",
      "        \"Solving an MDP by value iteration.\"\n",
      "        mdp = self.mdp\n",
      "        \n",
      "        Q = {}\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        state = initial_state\n",
      "        for i in range(0,iterations):\n",
      "            action = random.choice(mdp.actions(state))\n",
      "            p_next_state, next_state = T(state,action)\n",
      "            state_action = (state,action)\n",
      "          \n",
      "            maxQ_next = self.__state_actions(Q,next_state).max()\n",
      "            \n",
      "            if(deterministic):\n",
      "                Q[state_action] = R(state,action) + gamma*maxQ_next\n",
      "            else:\n",
      "                Q_sa = Q.get(state_action,0)\n",
      "                Q_sa += learning_rate*(R(state,action) + gamma*maxQ_next - Q.get(state_action,0))\n",
      "                Q[state_action] = Q_sa\n",
      "            \n",
      "            state = next_state\n",
      "            \n",
      "        \n",
      "        # We don't need to calculate the V-value, it's just easier to presentation purposes\n",
      "        V = dict([(s, self.__state_actions(Q,s).max()) for s in mdp.states])\n",
      "        pi = dict([(s, mdp.actions(s)[self.__state_actions(Q,s).argmax()]) for s in mdp.states])\n",
      "        \n",
      "        self.V = V\n",
      "        self.pi = pi  \n",
      "        \n",
      "        \n",
      "\n",
      "            \n",
      "        \n",
      "            \n",
      "    def __state_actions(self, Q, state):\n",
      "        mdp = self.mdp\n",
      "        state_actions = np.array([Q.get((state,a), 0) for a in mdp.actions(state)]) \n",
      "        #print state_actions\n",
      "        return state_actions\n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running the agent\n",
      "===\n",
      "* Both forms of Q-Learning can be used to learn the Q-values\n",
      "* Try increasing/decreasing the number of iterations for both cases\n",
      "* Compare both cases with the same number of training iterations - report what you observe\n",
      "* The Visualiser class helps us print the V-values of each - the lighter the colour of the cell, the higher the V-value is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Effort one, with deterministic Q-Learning\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "qa = QAgent(gmdp, 50)\n",
      "qa.QLearning(500, (0,0))\n",
      "values, policy  = qa.V, qa.pi\n",
      "\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'grid' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-51a7aec38fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Effort one, with deterministic Q-Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgmdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Effort two, with (real) Q-Learning\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "qa = QAgent(gmdp, 50)\n",
      "qa.QLearning(500, (0,0), deterministic = False, learning_rate = 0.01)\n",
      "values, policy  = qa.V, qa.pi\n",
      "\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise\n",
      "* We have implemented this:  \n",
      "\t* Q-learning, \n",
      "\t\t$Qs,a) \\gets Q(s,a) + \\eta\\left[R(s,a) + \\gamma \\max\\limits_{a' \\in A}Q(s',a') -\n",
      "Q(s,a) \\right]$\n",
      "* Try to implement this and see the differences\n",
      "\t* SARSA(0), \n",
      "\t\t$Q(s,a) \\gets Q(s,a) + \\eta\\left[R(s,a) + \\gamma Q(s',a') -\n",
      "Q(s,a) \\right]$\n",
      "\t* SARSA(1)/MC,\n",
      "\t\t$Q(s,a) \\gets Q(s,a) + \\eta\\left[\\mathrm{v_\\tau} - Q(s,a)\\right]$\n",
      "\t\t$\\mathrm{v_\\tau}  \\gets R(s,a)+\\gamma R(s',a')+...\\gamma^2 R(s'',a'') + \\gamma^{\\tau-1}R(s^\\tau, a^\\tau)$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}