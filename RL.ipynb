{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:7c6f12e824583211f1674f1f74d65e6aae47e87fde2f18f9a1697a14b804f245"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work with is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S \\times A$, $R(s,a)$ is a function that maps states/actions to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ after taking $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n",
      "    \n",
      "* This lab is just a demo of some of the principles we talked in class, now with transition loops\n",
      "* Most of the code is based on the book Artificial Intelligence: A Modern Approach \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615,AIM]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.99):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state, action):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state][action]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "        \n",
      "    def sample(state, action):\n",
      "        \"\"\"Sample from state action. Returns a new state\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Grid MDP\n",
      "===\n",
      "* We can now define a helper class called GridMDP, to help us encode a grid-like MDP\n",
      "* We will only use deterministic transitions\n",
      "    * ... but there are loops!\n",
      "* We will not use terminal states in our examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1, AIM].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.rows):\n",
      "            for y in range(self.cols):\n",
      "                self.reward[x, y] = grid[x][y]\n",
      "                if grid[x][y] is not None:\n",
      "                    self.states.add((x, y))\n",
      "        self.orig_grid = grid\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return 0.0, state\n",
      "        else:\n",
      "            return 1.0, self.go(state, action)\n",
      "        \n",
      "    def R(self, state, action):\n",
      "        \"Return a numeric reward for this state/action pair.\"\n",
      "        p_state_next, state_next = self.T(state,action)\n",
      "        if(state_next == state):\n",
      "            return 0\n",
      "        return self.reward[state_next]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating a sample GridMDP\n",
      "===\n",
      "* We can create now a simple MDP just by defining an 2D array structure\n",
      "* The values of the grid are the rewards at each (x,y) position"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "grid = [[-0.00, -0.00, -0.00, +1],\n",
      "        [-0.00, -0.00, -0.00, -1],\n",
      "        [-0.00, -0.00, -0.00, -0.00]]\n",
      "terminals = [(0, 3), (1, 3)]\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt(grid, fig = \"grid.png\")\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzFdTO4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fa836c71590>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q-Learning\n",
      "=== \n",
      "* We can now define a class for Q-learning agents\n",
      "* Notice how we have a deterministic option so the agent can choose how to update q-values\n",
      "* \"Real\" Q-learning not explicitly needed in our simple scenario, but interesting option"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "class QAgent():\n",
      "    \"\"\" A Q-learning iteration agent\n",
      "    \"\"\"\n",
      "    def __init__(self,mdp, k):\n",
      "        self.mdp = mdp\n",
      "        self.k = k\n",
      "    \n",
      "\n",
      "    def QLearning(self, iterations, initial_state, deterministic = True, learning_rate = 0.01):\n",
      "        \"Solving an MDP by value iteration.\"\n",
      "        mdp = self.mdp\n",
      "        \n",
      "        Q = {}\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        state = initial_state\n",
      "        for i in range(0,iterations):\n",
      "            action = random.choice(mdp.actions(state))\n",
      "            p_next_state, next_state = T(state,action)\n",
      "            state_action = (state,action)\n",
      "          \n",
      "            maxQ_next = self.__state_actions(Q,next_state).max()\n",
      "            \n",
      "            if(deterministic):\n",
      "                Q[state_action] = R(state,action) + gamma*maxQ_next\n",
      "            else:\n",
      "                Q_sa = Q.get(state_action,0)\n",
      "                Q_sa += learning_rate*(R(state,action) + gamma*maxQ_next - Q.get(state_action,0))\n",
      "                Q[state_action] = Q_sa\n",
      "            \n",
      "            state = next_state\n",
      "            \n",
      "        \n",
      "        # We don't need to calculate the V-value, it's just easier to presentation purposes\n",
      "        V = dict([(s, self.__state_actions(Q,s).max()) for s in mdp.states])\n",
      "        pi = dict([(s, mdp.actions(s)[self.__state_actions(Q,s).argmax()]) for s in mdp.states])\n",
      "        \n",
      "        self.V = V\n",
      "        self.pi = pi  \n",
      "        \n",
      "        \n",
      "\n",
      "            \n",
      "        \n",
      "            \n",
      "    def __state_actions(self, Q, state):\n",
      "        mdp = self.mdp\n",
      "        state_actions = np.array([Q.get((state,a), 0) for a in mdp.actions(state)]) \n",
      "        #print state_actions\n",
      "        return state_actions\n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running the agent\n",
      "===\n",
      "* Both forms of Q-Learning can be used to learn the Q-values\n",
      "* Try increasing/decreasing the number of iterations for both cases\n",
      "* Compare both cases with the same number of training iterations - report what you observe\n",
      "* The Visualiser class helps us print the V-values of each - the lighter the colour of the cell, the higher the V-value is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Effort one, with deterministic Q-Learning\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "qa = QAgent(gmdp, 50)\n",
      "qa.QLearning(500, (0,0))\n",
      "values, policy  = qa.V, qa.pi\n",
      "\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAADwCAYAAAB2b2N+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA2AS5QfAJMoPgEmUHwCT/hfFG/d4mCdlRgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fa85bd81850>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Effort two, with (real) Q-Learning\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "qa = QAgent(gmdp, 50)\n",
      "qa.QLearning(500, (0,0), deterministic = False, learning_rate = 0.01)\n",
      "values, policy  = qa.V, qa.pi\n",
      "\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAADyCAYAAAAvMlyUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYIEAAMFCQAGChIADBQkABj+DQRs2FCMNxrIAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fa83431a990>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise\n",
      "* We have implemented this:  \n",
      "\t* Q-learning, \n",
      "\t\t$Qs,a) \\gets Q(s,a) + \\eta\\left[R(s,a) + \\gamma \\max\\limits_{a' \\in A}Q(s',a') -\n",
      "Q(s,a) \\right]$\n",
      "* Try to implement these two and see the differences\n",
      "\t* SARSA(0), \n",
      "\t\t$Q(s,a) \\gets Q(s,a) + \\eta\\left[R(s,a) + \\gamma Q(s',a') -\n",
      "Q(s,a) \\right]$\n",
      "\t* SARSA(1)/MC,\n",
      "\t\t$Q(s,a) \\gets Q(s,a) + \\eta\\left[\\mathrm{v_\\tau} - Q(s,a)\\right]$\n",
      "\t\t$\\mathrm{v_\\tau}  \\gets R(s,a)+\\gamma R(s',a')+...\\gamma^2 R(s'',a'') + \\gamma^{\\tau-1}R(s^\\tau, a^\\tau)$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}