{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:7c6f12e824583211f1674f1f74d65e6aae47e87fde2f18f9a1697a14b804f245"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work with is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S$, $R(s,a)$ is a function that maps states/actions to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ after taking $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n",
      "    \n",
      "* This lab is just a demo of some of the principles we talked in class, now with transition loops\n",
      "* Most of the code is based on the book Artificial Intelligence: A Modern Approach \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615,AIM]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.99):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state, action):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state][action]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "        \n",
      "    def sample(state, action):\n",
      "        \"\"\"Sample from state action. Returns a new state\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Grid MDP\n",
      "===\n",
      "* We can now define a helper class called GridMDP, to help us encode a grid-like MDP\n",
      "* We will only use deterministic transitions\n",
      "    * ... but there are loops!\n",
      "* We will not use terminal states in our examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1, AIM].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.rows):\n",
      "            for y in range(self.cols):\n",
      "                self.reward[x, y] = grid[x][y]\n",
      "                if grid[x][y] is not None:\n",
      "                    self.states.add((x, y))\n",
      "        self.orig_grid = grid\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return 0.0, state\n",
      "        else:\n",
      "            return 1.0, self.go(state, action)\n",
      "        \n",
      "    def R(self, state, action):\n",
      "        \"Return a numeric reward for this state/action pair.\"\n",
      "        p_state_next, state_next = self.T(state,action)\n",
      "        if(state_next == state):\n",
      "            return 0\n",
      "        return self.reward[state_next]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating a sample GridMDP\n",
      "===\n",
      "* We can create now a simple MDP just by defining an 2D array structure\n",
      "* The values of the grid are the rewards at each (x,y) position"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "grid = [[-0.00, -0.00, -0.00, +1],\n",
      "        [-0.00, -0.00, -0.00, -1],\n",
      "        [-0.00, -0.00, -0.00, -0.00]]\n",
      "terminals = [(0, 3), (1, 3)]\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt(grid, fig = \"grid.png\")\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCcAGPwHyApPz55CSh0AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fc67e0d9410>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q-Learning\n",
      "=== \n",
      "* We can now define a class for Q-learning agents\n",
      "* Notice how we have a deterministic option so the agent can choose how to update q-values\n",
      "* \"Real\" Q-learning not explicitly needed in our simple scenario, but interesting option"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "class QAgent():\n",
      "    \"\"\" A Q-learning iteration agent\n",
      "    \"\"\"\n",
      "    def __init__(self,mdp, k):\n",
      "        self.mdp = mdp\n",
      "        self.k = k\n",
      "    \n",
      "\n",
      "    def QLearning(self, iterations, initial_state, deterministic = True, learning_rate = 0.01):\n",
      "        \"Solving an MDP by value iteration.\"\n",
      "        mdp = self.mdp\n",
      "        \n",
      "        Q = {}\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        state = initial_state\n",
      "        for i in range(0,iterations):\n",
      "            action = random.choice(mdp.actions(state))\n",
      "            p_next_state, next_state = T(state,action)\n",
      "            state_action = (state,action)\n",
      "          \n",
      "            maxQ_next = self.__state_actions(Q,next_state).max()\n",
      "            \n",
      "            if(deterministic):\n",
      "                Q[state_action] = R(state,action) + gamma*maxQ_next\n",
      "            else:\n",
      "                Q_sa = Q.get(state_action,0)\n",
      "                Q_sa += learning_rate*(R(state,action) + gamma*maxQ_next - Q.get(state_action,0))\n",
      "                Q[state_action] = Q_sa\n",
      "            \n",
      "            state = next_state\n",
      "            \n",
      "        \n",
      "        # We don't need to calculate the V-value, it's just easier to presentation purposes\n",
      "        V = dict([(s, self.__state_actions(Q,s).max()) for s in mdp.states])\n",
      "        pi = dict([(s, mdp.actions(s)[self.__state_actions(Q,s).argmax()]) for s in mdp.states])\n",
      "        \n",
      "        self.V = V\n",
      "        self.pi = pi  \n",
      "        \n",
      "        \n",
      "\n",
      "            \n",
      "        \n",
      "            \n",
      "    def __state_actions(self, Q, state):\n",
      "        mdp = self.mdp\n",
      "        state_actions = np.array([Q.get((state,a), 0) for a in mdp.actions(state)]) \n",
      "        #print state_actions\n",
      "        return state_actions\n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running the agent\n",
      "===\n",
      "* Both forms of Q-Learning can be used to learn the Q-values\n",
      "* Try increasing/decreasing the number of iterations for both cases\n",
      "* Compare both cases with the same number of training iterations - report what you observe\n",
      "* The Visualiser class helps us print the V-values of each - the lighter the colour of the cell, the higher the V-value is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Effort one, with deterministic Q-Learning\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "qa = QAgent(gmdp, 50)\n",
      "qa.QLearning(500, (0,0))\n",
      "values, policy  = qa.V, qa.pi\n",
      "\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAADwCAYAAAB2b2N+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPwAmUX4ATKL8AJhE+QEwifIDYNL/AQgx3l9HpJ61AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fc67def0ed0>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Effort two, with (real) Q-Learning\n",
      "gmdp = GridMDP(grid,terminals=[])\n",
      "qa = QAgent(gmdp, 50)\n",
      "qa.QLearning(500, (0,0), deterministic = False, learning_rate = 0.01)\n",
      "values, policy  = qa.V, qa.pi\n",
      "\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAADyCAYAAAAvMlyUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njoGjAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fc67e0c0a90>"
       ]
      }
     ],
     "prompt_number": 17
    }
   ],
   "metadata": {}
  }
 ]
}