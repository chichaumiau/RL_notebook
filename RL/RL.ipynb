{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:09c71b3ede20378d6e326e13a83c4f40abbef3b7930cc849f2b4e38e04254c5a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Reinforcement Learning\n",
      "===="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Outline\n",
      "---\n",
      "***\n",
      "* Part I Introduction\n",
      "    * Introduction & Motivation\n",
      "        * What is Reinforcement Learning (RL) ?\n",
      "        * Markov Decision Process (MDPs)\n",
      "    * Model-Based Reinforcement Learning\n",
      "        * Bellman Backups\n",
      "* Part II Iterative Methods\n",
      "    * Model-Based Reinforcement Learning\n",
      "        * Value Iteration \n",
      "        * Policy Iteration \n",
      "    * Model-Free Reinforcement Learning\n",
      "        * Monte Carlo Control\n",
      "        * SARSA\n",
      "        * Q-Learning\n",
      "* Part III \n",
      "    * Discussion, References, Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is Reinforcment Learning? \n",
      "---\n",
      "***\n",
      "* *Reinforcement learning is the study of how animals and artificial systems can learn to optimize their behavior in the face of rewards and punishments* -- Peter Dyan, Encyclopedia of Cognitive Science\n",
      "* Not Supervised Learning - the animal/agent is not provided with examples of optimal behaviour, it has to be discovered!\n",
      "* It subsumes most Artificial Intelligence problems. Forms the basis of most modern intelligent agents frameworks\n",
      "* Ideas drawn from a wide range of contexts, including psychology (e.g, Skinner's \"Operant Conditioning\") and Philosophy (e.g, \"Freedom, Equality, Property and Bentham\"), neuroscience, operations research  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* Play Backgammon/Chess/Go/Poker (at human or superhuman level)\n",
      "* Hellicopter Control\n",
      "* Learn how to walk/crawl/swim/cycle\n",
      "* Elevator Scheduling\n",
      "* Optimising an petreulem refinery \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work in is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S$, $R(s)$ is a function that maps states to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ if it takes action $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "\n",
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.99):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "        \n",
      "    def sample(state, action):\n",
      "        \"\"\"Sample from state action. Returns a new state\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Property and States\n",
      "---\n",
      "***\n",
      "* States represent sufficient statistics. \n",
      "* Markov Property ensures that we only care about the present in order to act, not past states\n",
      "* Think Tetris - All information are can be captured by a single screenshot\n",
      "\n",
      "1984 Original Version|1986 DOS Version \n",
      "-------------        | -------------\n",
      "<img src=\"250px-Tetris-VeryFirstVersion.png\" alt=\"Drawing\" style=\"width: 300px;\">           | <img src=\"250px-Tetris_DOS_1986.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
      "* States $s$ where there are no actions are called *terminal*(e.g., endgames) - there are other definitions as well\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents, Actions and Transitions\n",
      "---\n",
      "***\n",
      "* An agent is an entity capable of actions\n",
      "* An MDP can capture any environment that is inhabited either by exaclty one agent or that other agents are not adaptive\n",
      "* Notice how actions are described by the MDP, which captures the world dynamics, not the agent\n",
      "* In effect, the agent is just a \"brain in a vat\", an action-selector\n",
      "* The Agent perceives states/rewards and outputs actions\n",
      "* Transitions specify the effects of actions in the world (e.g.,  in Tetris, you push a button, the block spings)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Rewards and the Discount Factor\n",
      "---\n",
      "***\n",
      "* Rewards describe state preferences\n",
      "* Agent is happier in some states of the MDP (e.g. in Tetris when the block level is low - a fish in water)\n",
      "* Punishment is just low/negative reward\n",
      "* $\\gamma$\n",
      "    * the discount factor, describes the impact of time on rewards \n",
      "    * \"I want it now\", the lower $\\gamma$ is the less important future rewards are \n",
      "* There are no \"springs of rewards\" in the real world - \"human nature ?\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reward Schemes\n",
      "---\n",
      "***\n",
      "* Scoring in most video games\n",
      "* The distance a robot walked for a bipedal robot\n",
      "* The amount of food an animal eats\n",
      "* Money in modern societies\n",
      "* Army Medals (\"Gamification\") \n",
      "*  (- Fuel spend on a flight)(+ Distance Covered)\n",
      "* Cold/Hot "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Long Term Thinking\n",
      "---\n",
      "* It might be better to delay satisfaction\n",
      "* Immidiate reward is not allways the maximum reward\n",
      "* In some settings there are no immidiate rewards at all (e.g. some solitaire games) \n",
      "* MDPs and RL capture this\n",
      "* \"Not going out tonight, study\"\n",
      "* Long term investement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Policy\n",
      "---\n",
      "***\n",
      "* The MDP (the universe) is populated by an Agent (an actor)\n",
      "* You can take actions (e.g. move around, move blocks)\n",
      "* The type of actions you take under a state is called the *policy*\n",
      "* $\\pi_\\theta: S \\times A$, $\\pi_\\theta(s|a)$, a probabalistic mapping between states and actions, using parameters theta\n",
      "* Finding an optimal policy is *mostly* what the RL problem is all about\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Full Loop\n",
      "---\n",
      "***\n",
      "* See how the universe described by the MDP defines actions, not just states and transitions\n",
      "* An agent needs to action upon what it perceives\n",
      "* Notice the lack of body - \"brain in a vat\". Body is assumed to be part of the world. \n",
      "\n",
      "<img style=\"float:centre\" src=\"RL.png\">\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld\n",
      "---\n",
      "***\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agent Goals\n",
      "---\n",
      "***\n",
      "* The agents goal is to maximise its long term reward $J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum\\limits_{t=0}^\\infty{\\gamma^tR(s)}\\right]$\n",
      "\n",
      "* $\\underset{\\theta}{\\operatorname{argmax}}  J(\\theta)$\n",
      "* Risk Neutral Agent - think of the EagleWorld Example\n",
      "* Rewards can be anything, but most organisms receive rewards only in a very limited amount of states (e.g., fish in water)\n",
      "* What if your reward signal is money ?\n",
      "* Sociopathic, Egotistic, Greed-is-good Gordon Gecco\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Searching for a Good Policy\n",
      "---\n",
      "***\n",
      "* One can possibly search through all combinations of policies until he finds the best\n",
      "* Slow, does not work in larger MDPs\n",
      "* Exploration/Exploitation Dillema\n",
      "    * How much time/effort should be spend exploring for solutions\n",
      "    * How much time should be spend exploiting good solutions\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Based Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* ...also known as planning in certain contexts\n",
      "* Who was doing the thinking in the previous example (You? The eagle ?)  \n",
      "* An agent has access to model, i.e. has a copy of the MDP (the outside world) in its brain\n",
      "* Using that copy, it tries to \"think\" what is the best route of action\n",
      "* It than executes this policy on the real world MDP\n",
      "* You can't really copy the world inside your head, but you can copy the dynamics\n",
      "* \"This and that will happen if I push the chair\"\n",
      "* Thinking, introspection..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bellman Expectation Equations / Bellman Backups\n",
      "---\n",
      "***\n",
      "* There are the two most important functions related to an MDP \n",
      "* Recursive definitions\n",
      "* $ {V^\\pi (s) = R(s) + \\sum\\limits_{a \\in A}\\pi(s,a)\\left( \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')     \\right)}$ \n",
      "* ${Q^\\pi (s,a) =  \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a)\\left(R(s') +   \\sum\\limits_{a' \\in A}\\pi(a',s')Q(s',a')    \\right)}$ \n",
      "* Called Q-Value(s) and V-Value(s) respectively\n",
      "* They are also interrelated\n",
      "* $Q^\\pi(s,a) = \\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')$\n",
      "* $V^\\pi(s) =  R(s) + \\sum\\limits_{a' \\in A} \\pi(s,a) Q^\\pi(s,a)$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Optimal Policy and the Bellman Optimality Equation\n",
      "---\n",
      "***\n",
      "* An optimal policy can be defined in terms of Q-values\n",
      "* It is the policy that maximises Q values\n",
      "* $Q^*(s,a) = \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a)R(s')\\max\\limits_{a'}Q(s',a')$\n",
      "* $\\pi^*(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q^*(s,a)$} \\\\ 0, &  \\mbox{ otherwise} \\end{cases}$\n",
      "* $V^*(s) =  \\sum\\limits_{a' \\in A} \\pi^*(s,a) Q^\\pi(s,a)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Q_value(a, s, V, mdp):\n",
      "    \"The expected reward of doing action a in state s, according to the MDP and V.\"\n",
      "    return sum([p * V[s_prime] for (p, s_prime) in mdp.T(s, a)])\n",
      "\n",
      "def optimal_policy(mdp, V):\n",
      "    \"\"\"Given an MDP and a value function V, determine the best policy,\n",
      "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
      "    pi = {}\n",
      "    for s in mdp.states:\n",
      "        pi[s] = argmax(mdp.actions(s), lambda a:Q_value(a, s, V, mdp))\n",
      "    return pi\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Random Policy\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n",
      "<center><font color='red'>$\\pi(Flying, Attack\\_Boar) = 0.5, \\pi(Flying, Attack\\_Turtle) = 0.5$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$V^\\pi(Flying) = 0.5  Q^\\pi(Flying, Attack\\_Turtle) + 0.5  Q(Flying, Attack\\_Boar) = 3.5145 $</font></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Optimal Policy\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$\\pi^*(Flying, Attack\\_Boar) = 1$, $\\pi^*(Flying, Attack\\_Turtle) = 0$ </font></center>\n",
      "<center><font color='red'>$V^*(Flying) = 0.0  Q^\\pi(Flying) + 1.0  Q(Flying, Attack\\_Boar) = 4.455 $</font></center>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents Revisited\n",
      "---\n",
      "***\n",
      "* An Agent can be composed of a number of things\n",
      "  * A policy \n",
      "  * A Q-Value/and or V-Value Function\n",
      "  * A Model of the environment (the MDP)\n",
      "  * Inference/Learning Mechanisms\n",
      "  * ...\n",
      "* An agent has to be able to *create a policy* either on the fly or using Q-Values \n",
      "* The Model/Q/V-Values serve as intermediate points towards construcing a policy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Relationship to the rest of Machine Learning\n",
      "---\n",
      "***\n",
      "* How can one learn a model of the world ? \n",
      "    * Possibly by breaking it down into smaller, abstract chunks\n",
      "        * Unsupervised Learning\n",
      "    * ... and learning what effects ones actions have the envinroment\n",
      "        * Supervised Learning\n",
      "* RL weaves all fields of Machine Learning (and possibly Artificial Intelligence) into one coherent whole\n",
      "* The purpose of all learning is action!\n",
      "    * You need to be able to recognise faces so you can create state\n",
      "    * ... and act on it\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "*Review Questions*\n",
      "---\n",
      "***\n",
      "1. Give a brief, intuitive description of Reinforcment Learning\n",
      "2. Define a Markov Decision Process\n",
      "2. Define an Agent\n",
      "3. Define the policy of an agent\n",
      "5. What is an Optimal Policy ? \n",
      "4. Assume an MDP with with single state $s_0$ and two actions $a_0, a_1$. Assume a policy $\\pi(s_0|a_0) = 0.3$, $\\pi(s_0|a_1) = 0.7$. Briefly give an intuitive explanation of the policy in this setting. \n",
      "5. Assume an MDP with states $s_0, s_1, s_2$, $R(s_0) = 0$, $R(s_1) = 2$ and $R(s_2) = 1$ and actions $a_0,a_1$. Also assume transitions $T(s_1|s_0, a_0) = 1$, $T(s_2|s_0,a_1) = 1$. State $s_1,s_2$ are terminal  \n",
      "    1. What is $Q(s_0,a_0)$ and $Q(s_0,a_1)$ ? \n",
      "    1. What should be the policy $\\pi(s_0|a_0)$ and $\\pi(s_0|a_1)$ ? \n",
      "    1. What is $V(s_0)$ ? \n",
      "6. Think of a simple MDP and draw it. Does it have terminal states ? Can it be solved directly using Bellman Backups ? \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Recursion\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample.png\">\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$Q^*(Flying, Keep\\_Flying) = 0.99  (max ( Q^*(Flying, Attack\\_Turtle) , Q(Flying, Attack\\_Boar), Q(Flying, Keep\\_Flying))$</font></center>\n",
      "<center><font color='red'>$\\pi(Flying,Attack\\_Boar = 1)$</font></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GridWorld (1)\n",
      "---\n",
      "***\n",
      "* As we saw previously, when they are cycles in the MDP (which might happen quite often) it's not easy to calculate the Bellman backups directly\n",
      "* An alternative is to used a method called *Policy Iteration*\n",
      "* Let's imagine an MDP full of loops - GridWorld\n",
      "    * A simple world, partitioned in a grid\n",
      "    * Goal it to get to the upper right corner, and avoid the block right below it (both terminal states)\n",
      "    * Rewards -0.04 everywhere, except the two terminal states (+1, -1) respectively\n",
      "    * MDP Transtions as follow\n",
      "        * $T(s\\_action|\\cdot, s\\_action) = 0.8 $\n",
      "        * $T(left|\\cdot,s\\_action) = 0.1 $\n",
      "        * $T(right|\\cdot,s\\_action) = 0.1 $\n",
      " \n",
      "    \n",
      "\n",
      "    \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GridWorld (2)\n",
      "---\n",
      "***\n",
      "States/Rewards       | Actions | Transitions\n",
      "-------------        | --------| -------------\n",
      "<img src=\"grid.png\"> | left,right, up,down|  $T(s\\_action|\\cdot, s\\_action) = 0.8 \\\\ T(left|\\cdot,s\\_action) = 0.1\\\\ T(right|\\cdot,s\\_action) = 0.1 $"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "\n",
      "\n",
      "\n",
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.rows):\n",
      "            for y in range(self.cols):\n",
      "                self.reward[x, y] = grid[x][y]\n",
      "                if grid[x][y] is not None:\n",
      "                    self.states.add((x, y))\n",
      "        self.orig_grid = grid\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return [(0.0, state)]\n",
      "        else:\n",
      "            return [(0.8, self.go(state, action)),\n",
      "                    (0.1, self.go(state, turn_right(action))),\n",
      "                    (0.1, self.go(state, turn_left(action)))]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = [[-0.04, -0.04, -0.04, +1],\n",
      "        [-0.04, -0.04, -0.04, -1],\n",
      "        [-0.04, -0.04, -0.04, -0.04]]\n",
      "terminals = [(0, 3), (1, 3)]\n",
      "gmdp = GridMDP(grid,terminals=terminals)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt(grid, fig = \"grid.png\")\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7faddcffde90>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "(Modified) Policy Iteration\n",
      "---\n",
      "***\n",
      "* Policy Iteration is an iterative, model-based method for attacking any kind of MDP\n",
      "* It does not make much sense in the case of finite MDPS without loops, use backups\n",
      "* Loop until policy no further changes\n",
      "    * Policy Evaluation: Find V-Values given a policy - start with a random policy\n",
      "        * Loop over states for a fixed number of iterations\n",
      "        * For all states $s$\n",
      "            * $ {V^{\\pi_{n}} (s) = R(s) + \\sum\\limits_{a \\in A}\\pi_n(s,a)\\left( \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a) V^{\\pi_n}(s')     \\right)}$ \n",
      "        * Don't recurse until the end - just do it for the immidate next state!\n",
      "    * Policy Improvement: From new policy by acting greedily on Q-Values (infered from V-Values)\n",
      "        * $\\pi_{n+1}(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  \\sum\\limits_{s' \\in S} T(s'|s,a) V^{\\pi_{n}}(s')$} \\\\ 0, & \\mbox{ otherwise} \\end{cases}$\n",
      "    \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def policy_iteration(mdp):\n",
      "    \"Solve an MDP by policy iteration [Fig. 17.7]\"\n",
      "    V = dict([(s, 0) for s in mdp.states])\n",
      "    pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
      "    while True:\n",
      "        V1 = V.copy()\n",
      "        V1 = policy_evaluation(pi, V1, mdp)\n",
      "        unchanged = True\n",
      "        for s in mdp.states:   \n",
      "            a = argmax(mdp.actions(s), lambda a: Q_value(a,s,V1,mdp))\n",
      "            if a != pi[s]:\n",
      "                pi[s] = a\n",
      "                unchanged = False\n",
      "        if unchanged:\n",
      "            return V1, pi\n",
      "\n",
      "def policy_evaluation(pi, V, mdp, k=50):\n",
      "    \"\"\"Return V-Values, using an approximation (modified policy iteration).\"\"\"\n",
      "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "    for i in range(k):\n",
      "        for s in mdp.states:\n",
      "            V[s] = R(s) + gamma * sum([p * V[s1] for (p, s1) in T(s, pi[s])])\n",
      "    return V\n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gmdp = GridMDP(grid,terminals=terminals)\n",
      "values, policy  = policy_iteration(gmdp)\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI0mSJEmSJEmSJEmSJEmSJElSP/k/Kx1shyi1yzQAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7faddc6e8150>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Discovered Policy and Values\n",
      "----\n",
      "***\n",
      "States/Rewards       | Actions | Transitions\n",
      "-------------        | --------| -------------\n",
      "<img src=\"grid_solved.png\"> | left,right, up,down|  $T(s\\_action|\\cdot, s\\_action) = 0.8 \\\\ T(left|\\cdot,s\\_action) = 0.1\\\\ T(right|\\cdot,s\\_action) = 0.1 $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Free Methods\n",
      "---\n",
      "***\n",
      "* We have assumed the agent has a copy of the MDP (a *model*) in its head\n",
      "* This is rarely the case\n",
      "* Generaly the agent can only interact with the envinroment\n",
      "* i.e. the agent is imbued in a statistical bath of states, actions, trantisions and rewards\n",
      "* How can the agent act then ? \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (1)\n",
      "---\n",
      "***\n",
      "* This is the easiest, most sure to work method of all\n",
      "* High Variance\n",
      "* MC (Naive Version)\n",
      "    * Start at any state, initialise $Q(s,a)$ as you visit states/actions \n",
      "    * Act $\\epsilon$-greedily\n",
      "        * $\\pi^\\epsilon(s,a) = \\begin{cases} 1-\\epsilon + \\epsilon/|A| , & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q^*(s,a)$} \\\\ \\epsilon/|A|, & \\mbox{if } \\mbox{ otherwise} \\end{cases}$\n",
      "    * Wait until episode ends\n",
      "    * Add all reward you have seen so far to $v^i_t$ for episode $i$\n",
      "    * $Q(s,a) \\gets  \\sum\\limits_i^n\\frac{v^i_t}{n}$, where $n$ is the times a state is visited  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Monte Carlo Control (2)\n",
      "---\n",
      "***\n",
      "* $\\epsilon$-greedy means acting greedily $1-\\epsilon$, random otherwise\n",
      "* Since there the policy we follow will change over time, $Q(s,a)$ is not stationary\n",
      "* Better use a non-stationary mean\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}