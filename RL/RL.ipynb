{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:8daea14dd2a79b1d68bb5b650fcb261ec3a82a9ef3cf17dfc27ff3af6da48137"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Reinforcement Learning\n",
      "===="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Outline\n",
      "---\n",
      "***\n",
      "* Introduction & Motivation\n",
      "    * What is Reinforcement Learning (RL) ?\n",
      "    * Markov Decision Process (MDPs)\n",
      "    * Bellman Equations\n",
      "* Model-Based Reinforcement Learning\n",
      "    * Value Iteration \n",
      "    * Policy Iteration \n",
      "* Model-Free Reinforcement Learning\n",
      "    * Monte Carlo Control\n",
      "    * SARSA\n",
      "    * Q-Learning\n",
      "* Discussion, References, Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is Reinforcment Learning? \n",
      "---\n",
      "***\n",
      "* *Reinforcement learning is the study of how animals and artificial systems can learn to optimize their behavior in the face of rewards and punishments* -- Peter Dyan, Encyclopedia of Cognitive Science\n",
      "* Not Supervised Learning - the animal/agent is not provided with examples of optimal behaviour, it has to be discovered!\n",
      "* It subsumes most Artificial Intelligence problems. Forms the basis of most modern intelligent agents frameworks\n",
      "* Ideas drawn from a wide range of contexts, including psychology (e.g, Skinner's \"Operant Conditioning\") and Philosophy (e.g, \"Freedom, Equality, Property and Bentham\"). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* Play Backgammon/Chess/Go/Poker (at human or superhuman level)\n",
      "* Hellicopter Control\n",
      "* Learn how to walk/crawl/swim\n",
      "* Elevator Scheduling\n",
      "* Optimising an petreulem refinery \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work in is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S$, $R(s)$ is a function that maps states to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ if it takes action $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.9):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Property and States\n",
      "---\n",
      "***\n",
      "* States represent sufficient statistics. \n",
      "* Markov Property ensures that we only care about the present in order to act, not past states\n",
      "* Think Tetris - All information are can be captured by a single screenshot\n",
      "\n",
      "1984 Original Version|1986 DOS Version \n",
      "-------------        | -------------\n",
      "<img src=\"250px-Tetris-VeryFirstVersion.png\" alt=\"Drawing\" style=\"width: 300px;\">           | <img src=\"250px-Tetris_DOS_1986.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
      "* States $s$ where $\\forall a \\in A T(s'|s,a) = 0$ are called *terminal* or *absorbing* (e.g., endgames) \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents, Actions and Transitions\n",
      "---\n",
      "***\n",
      "* An agent is an entity capable of actions\n",
      "* An MDP can capture any environment that is inhabited either by exaclty one agent or that other agents are not adaptive\n",
      "* Notice how actions are described by the MDP, which captures the world dynamics, not the agent\n",
      "* In effect, the agent is just a \"brain in a vat\", an action-selector\n",
      "* The Agent perceives states/rewards and outputs actions\n",
      "* Transitions specify the effects of actions in the world (e.g.,  in Tetris, you push a button, the block spings)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Rewards and the Discount Factor\n",
      "---\n",
      "***\n",
      "* Rewards describe state preferences\n",
      "* Agent is happier in some states of the MDP (e.g. in Tetris when the block level is low - a fish in water)\n",
      "* Punishment is just low/negative reward\n",
      "* $\\gamma$\n",
      "    * the discount factor, describes the impact of time on rewards \n",
      "    * \"I want it now\", the lower $\\gamma$ is the less important future rewards are \n",
      "* There are no \"springs of rewards\" in the real world - \"human nature ?\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reward Schemes\n",
      "---\n",
      "***\n",
      "* Scoring in most video games\n",
      "* The distance a robot walked for a bipedal robot\n",
      "* The amount of food an animal eats\n",
      "* Money in modern societies\n",
      "* Army Medals (\"Gamification\") \n",
      "*  (- Fuel spend on a flight)(+ Distance Covered)\n",
      "* Cold/Hot "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Long Term Thinking\n",
      "---\n",
      "* It might be better to delay satisfaction\n",
      "* Immidiate reward is not allways the maximum reward\n",
      "* In some settings there are no immidiate rewards at all (e.g. some solitaire games) \n",
      "* MDPs and RL capture this\n",
      "* \"Not going out tonight, study\"\n",
      "* Long term investement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Policy\n",
      "---\n",
      "***\n",
      "* The MDP (the universe) is populated by an Agent (an actor)\n",
      "* You can take actions (e.g. move around, move blocks)\n",
      "* The type of actions you take under a state is called the *policy*\n",
      "* $\\pi_\\theta: S \\times A$, $\\pi_\\theta(s|a)$, a probabalistic mapping between states and actions, using parameters theta\n",
      "* Finding an optimal policy is *mostly* what the RL problem is all about\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Full Loop\n",
      "---\n",
      "***\n",
      "* See how the universe described by the MDP defines actions, not just states and transitions\n",
      "* An agent needs to action upon what it perceives\n",
      "* Notice the lack of body - \"brain in a vat\". Body is assumed to be part of the world. \n",
      "\n",
      "<img style=\"float:centre\" src=\"RL.png\">\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld\n",
      "---\n",
      "***\n",
      "<img style=\"float:left\" src=\"MDPExample.png\">\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agent Goals\n",
      "---\n",
      "***\n",
      "* The agents goal is to maximise its long term reward $J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum\\limits_{t=0}^\\infty{\\gamma^tR(s)}\\right]$\n",
      "\n",
      "* $\\underset{\\theta}{\\operatorname{argmax}}  J(\\theta)$\n",
      "* Risk Neutral Agent - think of the EagleWorld Example\n",
      "* Rewards can be anything, but most organisms receive rewards only in a very limited amount of states (e.g., fish in water)\n",
      "* What if your reward signal is money ?\n",
      "* Sociopathic, Egotistic, Greed-is-good Gordon Gecco\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Searching for a Good Policy\n",
      "---\n",
      "***\n",
      "* One can possibly search through all combinations of policies until he finds the best\n",
      "* Slow, does not work in larger MDPs\n",
      "* Exploration/Exploitation Dillema\n",
      "    * How much time/effort should be spend exploring for solutions\n",
      "    * How much time should be spend exploiting good solutions\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bellman Expectation Equations\n",
      "---\n",
      "***\n",
      "* There are the two most important functions related to an MDP \n",
      "* Recursive definitions\n",
      "* $ {V^\\pi (s) = R(s) + \\sum\\limits_{a \\in A}\\pi(s|a)\\left( \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')     \\right)}$ \n",
      "* ${Q^\\pi (s,a) =  \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a)R(s')\\left(   \\sum\\limits_{a' \\in A}\\pi(a',s')Q(s',a')    \\right)}$ \n",
      "* Called Q-Value(s) and V-Value(s) respectively\n",
      "* They are also interrelated\n",
      "* $Q^\\pi(s,a) = \\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')$\n",
      "* $V^\\pi(s) =  \\sum\\limits_{a' \\in A} \\pi(s|a) Q^\\pi(s,a)$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Optimal Policy and the Bellman Optimality Equation\n",
      "---\n",
      "***\n",
      "* An optimal policy can be defined in terms of Q-values\n",
      "* It is the policy that maximises Q values\n",
      "* $Q^*(s,a) = \\max\\limits_\\pi Q(s,a)$\n",
      "* $Q^*(s,a) = \\gamma\\sum\\limits_{s' \\in S} T(s'|s,a)R(s')\\max\\limits_a'Q(s',a')$\n",
      "* $\\pi^*(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q^*(s,a)$} \\\\ 0, & \\mbox{if } \\mbox{ otherwise} \\end{cases}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Q_value(a, s, V, mdp):\n",
      "    \"The expected reward of doing action a in state s, according to the MDP and V.\"\n",
      "    return sum([p * V[s_prime] for (p, s_prime) in mdp.T(s, a)])\n",
      "\n",
      "def optimal_policy(mdp, V):\n",
      "    \"\"\"Given an MDP and a value function V, determine the best policy,\n",
      "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
      "    pi = {}\n",
      "    for s in mdp.states:\n",
      "        pi[s] = argmax(mdp.actions(s), lambda a:Q_value(a, s, V, mdp))\n",
      "    return pi\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld (2)\n",
      "---\n",
      "***\n",
      "<img style=\"float:left\" src=\"MDPExample.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents Revisited\n",
      "---\n",
      "***\n",
      "* An Agent can be composed of a number of things\n",
      "  * A policy \n",
      "  * A Q-Value/and or V-Value Function\n",
      "  * A Model of the environment (the MDP)\n",
      "* An agent has to be able to create a policy either on the fly or using Q-Values \n",
      "* The Model/Q/V-Values serve as intermediate points towards construcing a policy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Relationship to the rest of Machine Learning\n",
      "---\n",
      "***\n",
      "* How can one learn a model of the world ? \n",
      "    * Possibly by breaking it down into smaller, abstract chunks\n",
      "        * Unsupervised Learning\n",
      "    * ... and learning what effects ones actions have the envinroment\n",
      "        * Supervised Learning\n",
      "* RL weaves all fields of Machine Learning (and possibly Artificial Intelligence) into one coherent whole\n",
      "* The purpose of all learning is action!\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "*Review Questions*\n",
      "---\n",
      "***\n",
      "1. Give a brief, intuitive description of Reinforcment Learning. \n",
      "2. Define a Markov Decision Process.\n",
      "3. Define the policy of an agent.  \n",
      "4. Assume an MDP with with single state $s_0$ and two actions $a_0, a_1$. Assume a policy $\\pi(s_0|a_0) = 0.3$, $\\pi(s_0|a_1) = 0.7$. Briefly give an intuitive explanation of the policy in this setting. \n",
      "5. Assume an MDP with states $s_0, s_1, s_2$, $R(s_0) = 0$, $R(s_1) = 1$ and $R(s_2) = 0$ and actions $a_0,a_1$. Also assume transitions $T(s_1|s_0, a_0) = 1$, $T(s_2|s_0,a_1) = 1 $ and $T(s_1|s_1, a_0) = T(s_2|s_2, a_0) = 1$  \n",
      "    1. What should be the policy $\\pi(s_0|a_0)$ and $\\pi(s_0|a_1)$ ? \n",
      "    2. How are states $s_0$ and $s_1$ called?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Based Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* ...also known as planning in certain contexts\n",
      "* An agent has access to model, i.e. has a copy of the MDP (the outside world) in its brain\n",
      "* Using that copy, it tries to \"think\" what is the best route of action\n",
      "* It than executes this policy on the real world MDP\n",
      "* You can't really copy the world inside your head, but you can copy the dynamics\n",
      "* \"This and that will happen if I push the chair\"\n",
      "* Thinking, introspection..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "\n",
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        grid.reverse() ## because we want row 0 on bottom, not on top\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.cols):\n",
      "            for y in range(self.rows):\n",
      "                self.reward[x, y] = grid[y][x]\n",
      "                if grid[y][x] is not None:\n",
      "                    self.states.add((x, y))\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return [(0.0, state)]\n",
      "        else:\n",
      "            return [(0.8, self.go(state, action)),\n",
      "                    (0.1, self.go(state, turn_right(action))),\n",
      "                    (0.1, self.go(state, turn_left(action)))]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    def to_grid(self, mapping):\n",
      "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
      "        return list(reversed([[mapping.get((x,y), None)\n",
      "                               for x in range(self.cols)]\n",
      "                              for y in range(self.rows)]))\n",
      "\n",
      "    def to_arrows(self, policy):\n",
      "        chars = {(1, 0):'>', (0, 1):'^', (-1, 0):'<', (0, -1):'v', None: '.'}\n",
      "        return self.to_grid(dict([(s, chars[a]) for (s, a) in policy.items()]))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = [[-0.04, -0.04, -0.04, +1],\n",
      "        [-0.04, -0.04, -0.04, -1],\n",
      "        [-0.04, -0.04, -0.04, -0.04]]\n",
      "terminals = [(3, 2), (3, 1)]\n",
      "gmdp = GridMDP(grid,terminals=terminals)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "#import numpy as np\n",
      "plt.clf()\n",
      "# Choose gray colormap\n",
      "cmap = plt.get_cmap(\"gist_gray\")\n",
      "# Set ticks to the way we like them\n",
      "plt.xticks(np.arange(0, len(grid[0]), 1.0))\n",
      "plt.yticks(np.arange(0, len(grid), 1.0))\n",
      "extent = np.array([- 0.5,len(grid[0])- 0.5, len(grid) - 0.5, - 0.5]) \n",
      "# create the image\n",
      "cax = plt.imshow(grid, cmap = cmap, interpolation=\"nearest\", aspect='equal', extent=extent, origin='lower')\n",
      "# Add colorbar\n",
      "cbar = plt.colorbar(cax, ticks=[-1, 0, 1])\n",
      "plt.arrow(0,0,0.2,0.0)\n",
      "plt.arrow(0,1,-0.2,0.0)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f738c039e10>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def value_iteration(mdp, epsilon=0.001):\n",
      "    \"Solving an MDP by value iteration. [Fig. 17.4]\"\n",
      "    U1 = dict([(s, 0) for s in mdp.states])\n",
      "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "    while True:\n",
      "        U = U1.copy() \n",
      "        delta = 0\n",
      "        for s in mdp.states:\n",
      "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
      "                                        for a in mdp.actions(s)])\n",
      "            delta = max(delta, abs(U1[s] - U[s]))\n",
      "        if delta < epsilon * (1 - gamma) / gamma:\n",
      "             return U\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "policy = value_iteration(gmdp)\n",
      "gmdp.to_arrows(best_policy(gmdp, policy))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'gmdp' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-dbb946b85988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgmdp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_arrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgmdp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'gmdp' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def policy_iteration(mdp):\n",
      "    \"Solve an MDP by policy iteration [Fig. 17.7]\"\n",
      "    U = dict([(s, 0) for s in mdp.states])\n",
      "    pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
      "    while True:\n",
      "        U = policy_evaluation(pi, U, mdp)\n",
      "        unchanged = True\n",
      "        for s in mdp.states:\n",
      "            a = argmax(mdp.actions(s), lambda a: expected_utility(a,s,U,mdp))\n",
      "            if a != pi[s]:\n",
      "                pi[s] = a\n",
      "                unchanged = False\n",
      "        if unchanged:\n",
      "            return pi\n",
      "\n",
      "def policy_evaluation(pi, U, mdp, k=20):\n",
      "    \"\"\"Return an updated utility mapping U from each state in the MDP to its \n",
      "    utility, using an approximation (modified policy iteration).\"\"\"\n",
      "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "    for i in range(k):\n",
      "        for s in mdp.states:\n",
      "            U[s] = R(s) + gamma * sum([p * U[s] for (p, s1) in T(s, pi[s])])\n",
      "    return U"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}