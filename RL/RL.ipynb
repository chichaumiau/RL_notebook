{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:7c6f12e824583211f1674f1f74d65e6aae47e87fde2f18f9a1697a14b804f245"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<center> Reinforcement Learning </center> \n",
      "====\n",
      "<center> CE802 </center> \n",
      "---\n",
      "<center> **Spyridon Samothrakis** </center> "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Outline\n",
      "---\n",
      "***\n",
      "* Part I Introduction \n",
      "    * Introduction & Motivation\n",
      "    * Markov Decision Process (MDPs)\n",
      "    * Model-Based Reinforcement Learning\n",
      "        * Bellman Backups\n",
      "* Part II Iterative Methods\n",
      "    * Model-Based Reinforcement Learning\n",
      "        * Value Iteration \n",
      "        * Policy Iteration \n",
      "    * Model-Free Reinforcement Learning\n",
      "        * Monte Carlo Control\n",
      "        * SARSA\n",
      "        * Q-Learning\n",
      "* Part III \n",
      "    * Discussion, References, Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is Reinforcment Learning? \n",
      "---\n",
      "***\n",
      "* *Reinforcement learning is the study of how animals and artificial systems can learn to optimize their behavior in the face of rewards and punishments* -- Peter Dyan, Encyclopedia of Cognitive Science\n",
      "* Not Supervised Learning - the animal/agent is not provided with examples of optimal behaviour, it has to be discovered!\n",
      "* It subsumes most Artificial Intelligence problems \n",
      "* Forms the basis of most modern intelligent agent frameworks\n",
      "* Ideas drawn from a wide range of contexts, including psychology (e.g, Skinner's \"Operant Conditioning\") and Philosophy (e.g, Marx' \"Freedom, Equality, Property and Bentham\"), neuroscience, operations research  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* Play Backgammon/Chess/Go/Poker (at human or superhuman level)\n",
      "* Hellicopter Control\n",
      "* Learn how to walk/crawl/swim/cycle\n",
      "* Elevator Scheduling\n",
      "* Optimising an petreulem refinery \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work with is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S$, $R(s)$ is a function that maps states to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ after taking $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615,AIM]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.99):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "        \n",
      "    def sample(state, action):\n",
      "        \"\"\"Sample from state action. Returns a new state\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Property and States\n",
      "---\n",
      "***\n",
      "* States represent sufficient statistics. \n",
      "* Markov Property ensures that we only care about the present in order to act - we can safely ignore past states\n",
      "* Think Tetris - All information are can be captured by a single screenshot\n",
      "\n",
      "1984 Original Version|1986 DOS Version \n",
      "-------------        | -------------\n",
      "<img src=\"250px-Tetris-VeryFirstVersion.png\" alt=\"Drawing\" style=\"width: 300px;\">           | <img src=\"250px-Tetris_DOS_1986.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
      "* States $s$ where there are no actions are called *terminal*(e.g., endgames) - there are other definitions as well\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents, Actions and Transitions\n",
      "---\n",
      "***\n",
      "* An agent is an entity capable of actions\n",
      "* An MDP can capture any environment that is inhabited either by exaclty one agent or that other agents are not adaptive\n",
      "* Notice how actions are described by the MDP, which captures the world dynamics, not the agent\n",
      "* In effect, the agent is just a \"brain in a vat\"\n",
      "* The agent perceives states/rewards and outputs actions\n",
      "* Transitions specify the effects of actions in the world (e.g.,  in Tetris, you push a button, the block spins)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Rewards and the Discount Factor\n",
      "---\n",
      "***\n",
      "* Rewards describe state preferences\n",
      "* Agent is happier in some states of the MDP (e.g., in Tetris when the block level is low, a fish in water)\n",
      "* Punishment is just low/negative reward\n",
      "* $\\gamma$\n",
      "    * the discount factor, describes the impact of time on rewards \n",
      "    * \"I want it now\", the lower $\\gamma$ is the less important future rewards are \n",
      "* There are no \"springs/wells of rewards\" in the real world - \"human nature\" ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reward Schemes\n",
      "---\n",
      "***\n",
      "* Scoring in most video games\n",
      "* The distance a robot walked for a bipedal robot\n",
      "* The amount of food an animal eats\n",
      "* Money in modern societies\n",
      "* Army Medals (\"Gamification\") \n",
      "*  (- Fuel spend on a flight)(+ Distance Covered)\n",
      "* Cold/Hot "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Long Term Thinking\n",
      "---\n",
      "* It might be better to delay satisfaction\n",
      "* Immidiate reward is not allways the maximum reward\n",
      "* In some settings there are no immidiate rewards at all (e.g. most solitaire games) \n",
      "* MDPs and RL capture this\n",
      "* \"Not going out tonight, study\"\n",
      "* Long term investement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Policy\n",
      "---\n",
      "***\n",
      "* The MDP (the world) is populated by an agent (an actor)\n",
      "* You can take actions (e.g., move around, move blocks)\n",
      "* The type of actions you take under a state is called the *policy*\n",
      "* $\\pi: S \\times A$, $\\pi(s,a) = P(s|a)$, a probabalistic mapping between states and actions\n",
      "* Finding an optimal policy is *mostly* what the RL problem is all about\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Full Loop\n",
      "---\n",
      "***\n",
      "* See how the universe described by the MDP defines actions, not just states and transitions\n",
      "* An agent needs to action upon what it perceives\n",
      "* Notice the lack of body - \"brain in a vat\". Body is assumed to be part of the world. \n",
      "\n",
      "<img style=\"float:centre\" src=\"RL.png\">\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld\n",
      "---\n",
      "***\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agent Goals\n",
      "---\n",
      "***\n",
      "* The agents goal is to maximise its long term reward $ V^\\pi(s) =  \\mathbb{E}_{\\pi}\\left[\\sum\\limits_{t=0}^\\infty{\\gamma^tR \\left( s^{t} \\right) }\\right]$\n",
      "\n",
      "* $\\pi^* = \\underset{\\pi}{\\operatorname{argmax}}  V^\\pi(s_0)$\n",
      "* Risk Neutral Agent - think of the EagleWorld Example\n",
      "* Rewards can be anything, but most organisms receive rewards only in a very limited amount of states (e.g., fish in water)\n",
      "* What if your reward signal is money ?\n",
      "    * Sociopathic, Egotistic, Greed-is-good Gordon Gecco\n",
      "    * No concept of \"Externalities\" - agents might wreak havoc for marginal reward increases\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Searching for a Good Policy\n",
      "---\n",
      "***\n",
      "* One can possibly search through all combinations of policies until he finds the best\n",
      "* Slow, does not work in larger MDPs\n",
      "* Exploration/Exploitation Dillema\n",
      "    * How much time/effort should be spend exploring for solutions\n",
      "    * How much time should be spend exploiting good solutions\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Based Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* ...also known as planning in certain contexts\n",
      "* Who was doing the thinking in the previous example (You? The eagle ?)  \n",
      "* An agent has access to model, i.e., has a copy of the MDP (the outside world) in its mind\n",
      "* Using that copy, it tries to \"think\" what is the best route of action\n",
      "* It than executes this policy on the real world MDP\n",
      "* You can't really copy the world inside your head, but you can copy the dynamics\n",
      "* \"This and that will happen if I push the chair\"\n",
      "* Thinking, introspection..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bellman Expectation Equations / Bellman Backups\n",
      "---\n",
      "***\n",
      "* There are the two most important functions related to an MDP \n",
      "* $V(s) = \\mathbb{E}_{\\pi}\\left[\\sum\\limits_{t=0}^\\infty{\\gamma^tR \\left( s^{t} \\right) }\\right]$\n",
      "* Recursive definitions\n",
      "* $ {V^\\pi (s) = R(s) + \\gamma\\sum\\limits_{a \\in A}\\pi(s,a)\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')     \\right)}$ \n",
      "* ${Q^\\pi (s,a) =  \\sum\\limits_{s' \\in S} T(s'|s,a)\\left(R(s') +   \\gamma\\sum\\limits_{a' \\in A}\\pi(a',s')Q(s',a')    \\right)}$ \n",
      "* **Called  V-Value(s) (*state-value function*) and Q-Value(s) (*state-action value function*) respectively**\n",
      "* They are also interrelated\n",
      "* $Q^\\pi(s,a) = \\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')$\n",
      "* $V^\\pi(s) =  R(s) + \\gamma\\sum\\limits_{a' \\in A} \\pi(s,a) Q^\\pi(s,a)$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Optimal Policy and the Bellman Optimality Equation\n",
      "---\n",
      "***\n",
      "* An optimal policy can be defined in terms of Q-values\n",
      "* It is the policy that maximises Q values\n",
      "* ${Q^* (s,a) =  \\sum\\limits_{s' \\in S} T(s'|s,a)\\left(R(s') +   \\gamma\\max\\limits_{a' \\in A}\\pi(a',s')Q(s',a')    \\right)}$ \n",
      "* $ {V^* (s) = R(s) + \\gamma\\max\\limits_{a \\in A}\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V^*(s')     \\right)}$ \n",
      "* $\\pi^*(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q^*(s,a)$} \\\\ 0, &  \\mbox{ otherwise} \\end{cases}$\n",
      "* $V^*(s) = R(s) +  \\gamma\\sum\\limits_{a' \\in A} \\pi^*(s,a) Q^{*}(s,a) = R(s) + \\gamma\\max\\limits_{a' \\in A}  Q^{*}(s,a)$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Q_value(a, s, V, mdp):\n",
      "    \"The expected reward of doing action a in state s, according to the MDP and V.\"\n",
      "    return sum([p * V[s_prime] for (p, s_prime) in mdp.T(s, a)])\n",
      "\n",
      "def optimal_policy(mdp, V):\n",
      "    \"\"\"Given an MDP and a value function V, determine the best policy,\n",
      "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
      "    pi = {}\n",
      "    for s in mdp.states:\n",
      "        pi[s] = argmax(mdp.actions(s), lambda a:Q_value(a, s, V, mdp))\n",
      "    return pi\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Random Policy\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n",
      "<center><font color='red'>$\\pi(Flying, Attack\\_Boar) = 0.5, \\pi(Flying, Attack\\_Turtle) = 0.5$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$V^\\pi(Flying) = 0.5  Q^\\pi(Flying, Attack\\_Turtle) + 0.5  Q(Flying, Attack\\_Boar) = 3.5145 $</font></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Optimal Policy\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$\\pi^*(Flying, Attack\\_Boar) = 1$, $\\pi^*(Flying, Attack\\_Turtle) = 0$ </font></center>\n",
      "<center><font color='red'>$V^*(Flying) = 0.0  Q^\\pi(Flying) + 1.0  Q(Flying, Attack\\_Boar) = 4.455 $</font></center>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents Revisited\n",
      "---\n",
      "***\n",
      "* An Agent can be composed of a number of things\n",
      "  * A policy \n",
      "  * A Q-Value/and or V-Value Function\n",
      "  * A Model of the environment (the MDP)\n",
      "  * Inference/Learning Mechanisms\n",
      "  * ...\n",
      "* An agent has to be able to *create a policy* either on the fly or using Q-Values \n",
      "* The Model/Q/V-Values serve as intermediate points towards construcing a policy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Relationship to the rest of Machine Learning\n",
      "---\n",
      "***\n",
      "* How can one learn a model of the world ? \n",
      "    * Possibly by breaking it down into smaller, abstract chunks\n",
      "        * Unsupervised Learning\n",
      "    * ... and learning what effects ones actions have the envinroment\n",
      "        * Supervised Learning\n",
      "* RL weaves all fields of Machine Learning (and possibly Artificial Intelligence) into one coherent whole\n",
      "* The purpose of all learning is action!\n",
      "    * You need to be able to recognise faces so you can create state\n",
      "    * ... and act on it\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "*Review Questions*\n",
      "---\n",
      "***\n",
      "1. Give a brief, intuitive description of Reinforcment Learning\n",
      "2. Define a Markov Decision Process\n",
      "2. Define an Agent\n",
      "3. Define the policy of an agent\n",
      "5. What is an Optimal Policy ? \n",
      "4. Assume an MDP with with single state $s_0$ and two actions $a_0, a_1$. Assume a policy $\\pi(s_0|a_0) = 0.3$, $\\pi(s_0|a_1) = 0.7$. Briefly give an intuitive explanation of the policy in this setting. \n",
      "5. Assume an MDP with states $s_0, s_1, s_2$, $R(s_0) = 0$, $R(s_1) = 2$ and $R(s_2) = 1$ and actions $a_0,a_1$. Also assume transitions $T(s_1|s_0, a_0) = 1$, $T(s_2|s_0,a_1) = 1$. State $s_1,s_2$ are terminal  \n",
      "    1. What is $Q(s_0,a_0)$ and $Q(s_0,a_1)$ ? \n",
      "    1. What should be the policy $\\pi(s_0|a_0)$ and $\\pi(s_0|a_1)$ ? \n",
      "    1. What is $V(s_0)$ ? \n",
      "6. Think of a simple MDP and draw it. Does it have terminal states ? Can it be solved directly using Bellman Backups ? \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Recursion\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample.png\">\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$Q^*(Flying, Keep\\_Flying) = 0.99  (max ( Q^*(Flying, Attack\\_Turtle) , Q(Flying, Attack\\_Boar), Q(Flying, Keep\\_Flying))$</font></center>\n",
      "<center><font color='red'>$\\pi(Flying,Attack\\_Boar = 1)$</font></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GridWorld (1)\n",
      "---\n",
      "***\n",
      "* As we saw previously, when there are cycles in the MDP (which might happen quite often) it's not easy to calculate the Bellman backups directly\n",
      "* A number of methods to overcome this\n",
      "* Let's imagine an MDP full of loops - GridWorld\n",
      "    * A simple world, partitioned in a grid\n",
      "    * Goal it to get to the upper right corner, and avoid the block right below it (both terminal states)\n",
      "    * Rewards -0.04 everywhere, except the two terminal states (+1, -1) respectively\n",
      "    * MDP Transtions as follow\n",
      "        * $T(s\\_action|\\cdot, s\\_action) = 0.8 $\n",
      "        * $T(s\\_left|\\cdot,s\\_action) = 0.1 $\n",
      "        * $T(s\\_right|\\cdot,s\\_action) = 0.1 $\n",
      " \n",
      "    \n",
      "\n",
      "    \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GridWorld (2)\n",
      "---\n",
      "***\n",
      "States/Rewards       | Actions  \n",
      "-------------        | --------\n",
      "<img src=\"grid.png\"> | left, right, up, down\n",
      "\n",
      "<p style='text-align: center;'>**Transitions**</p>\n",
      "<p style='text-align: center;'>$T(s\\_action|\\cdot, s\\_action) = 0.8 \\\\ T(s\\_left|\\cdot,s\\_action) = 0.1\\\\ T(s\\_right|\\cdot,s\\_action) = 0.1 $</p>\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "\n",
      "\n",
      "\n",
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.rows):\n",
      "            for y in range(self.cols):\n",
      "                self.reward[x, y] = grid[x][y]\n",
      "                if grid[x][y] is not None:\n",
      "                    self.states.add((x, y))\n",
      "        self.orig_grid = grid\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return [(0.0, state)]\n",
      "        else:\n",
      "            return [(0.8, self.go(state, action)),\n",
      "                    (0.1, self.go(state, turn_right(action))),\n",
      "                    (0.1, self.go(state, turn_left(action)))]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = [[-0.04, -0.04, -0.04, +1],\n",
      "        [-0.04, -0.04, -0.04, -1],\n",
      "        [-0.04, -0.04, -0.04, -0.04]]\n",
      "terminals = [(0, 3), (1, 3)]\n",
      "gmdp = GridMDP(grid,terminals=terminals)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt(grid, fig = \"grid.png\")\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f5b9c7d7c10>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Value Iteration\n",
      "---\n",
      "***\n",
      "* There are iterative methods that can get us the value\n",
      "* One of them is termed value iteration\n",
      "    * Start with some random V_0(s)\n",
      "    * Proceed with calculating \n",
      "        * $ {V_{n+1} (s) =  R(s) + \\gamma\\max\\limits_{a \\in A}\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V_n(s')     \\right)}$\n",
      "        * Use only the next s', **do not recurse**\n",
      "    * Repeat\n",
      "* Such methods are called \"Dynamic Programming\"\n",
      "    * Do not make much sense in the case of finite MDPS without loops, use backups\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "(Modified) Policy Iteration\n",
      "---\n",
      "***\n",
      "* Policy Iteration is an iterative, model-based method for attacking any kind of MDP\n",
      "\n",
      "* Loop until policy no further changes\n",
      "    * Policy Evaluation: Find V-Values given a policy - start with a random policy\n",
      "        * Loop over states for a fixed number of iterations $k$\n",
      "        * For all states $s$\n",
      "            * $ {V^{\\pi_{n}} (s) =  R(s) + \\gamma\\sum\\limits_{a \\in A}\\pi_n(s,a)\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V^{\\pi_{n-1}}(s')     \\right)}$ \n",
      "        * Don't recurse until the end - just do it for the immidate next state!\n",
      "    * Policy Improvement: From new policy by acting greedily on Q-Values (infered from V-Values)\n",
      "        * $\\pi_{n+1}(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  \\sum\\limits_{s' \\in S} T(s'|s,a) V^{\\pi_{n}}(s')$} \\\\ 0, & \\mbox{ otherwise} \\end{cases}$\n",
      "    \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class PIAgent():\n",
      "    \"\"\" A policy iteration agent\n",
      "    \"\"\"\n",
      "    def __init__(self,mdp, k):\n",
      "        self.mdp = mdp\n",
      "        self.k = k\n",
      "    \n",
      "    \n",
      "    def policyIteration(self):\n",
      "        \"\"\" Solve an MDP by policy iteration [Fig. 17.7, AIM], \n",
      "            A\n",
      "        \"\"\"\n",
      "        mdp = self.mdp\n",
      "        k = self.k\n",
      "        V = dict([(s, 0) for s in mdp.states])\n",
      "        pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
      "        while True:\n",
      "            V1 = V.copy()\n",
      "            V1 = self.policyEvaluation(pi, V1, mdp, k)\n",
      "            unchanged = True\n",
      "            for s in mdp.states:   \n",
      "                a = argmax(mdp.actions(s), lambda a: Q_value(a,s,V1,mdp))\n",
      "                if a != pi[s]:\n",
      "                    pi[s] = a\n",
      "                    unchanged = False\n",
      "            if unchanged:\n",
      "                self.V = V1\n",
      "                self.pi = pi\n",
      "                return\n",
      "\n",
      "    def policyEvaluation(self, pi, V, mdp,k ):\n",
      "        \"\"\"Return V-Values, using an approximation (modified policy iteration).\"\"\"\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        for i in range(k):\n",
      "            for s in mdp.states:\n",
      "                V[s] = R(s) + gamma * sum([p * V[s1] for (p, s1) in T(s, pi[s])])\n",
      "        return V\n",
      "    \n",
      "    def valueIteration(epsilon=0.001):\n",
      "        \"Solving an MDP by value iteration.\"\n",
      "        V1 = dict([(s, 0) for s in mdp.states])\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        while True:\n",
      "            V = V1.copy() \n",
      "            delta = 0\n",
      "            for s in mdp.states:\n",
      "                V1[s] = R(s) + gamma * max([sum([p * V[s1] for (p, s1) in T(s, a)])\n",
      "                                            for a in mdp.actions(s)])\n",
      "                delta = max(delta, abs(V1[s] - V[s]))\n",
      "            if delta < epsilon * (1 - gamma) / gamma:\n",
      "                self.V = V \n",
      "                return \n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gmdp = GridMDP(grid,terminals=terminals)\n",
      "pia = PIAgent(gmdp, 50)\n",
      "pia.policyIteration()\n",
      "values, policy  = pia.V, pia.pi\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADdJJREFUeJzt3G+MVXV+x/H38C/dhgooJMIIGTWzmyUCYytS7OIwW0xp\nVsFdoC6s3WKfuWuWGrcxdp/QR00fmOyaTc2mQYEaYBF0xLpgNMzgIqhgUf6pgQCCwJISpiLaVOnc\nPrgjy26BufO958zvnOH9Sg7MDOee++EyfOZ77vndC5IkSZIkSZIkSZIkSZIkScrQU8ApYE/qIJJU\nJjOAW7E8JanPmsigPAfVn0OSrj6WpyQFWJ6SSmXUqFEVoNbtbF45huR1YEnKQ1dXF93d3TXtO2jQ\noD/KK4eTp6TSqVQqNW2XsBrYBnwVOAY8EM3QEL2hJCVSOX/+fE07DhkyBHLqOU/bJZVOraftebI8\nJZXOZU7J+5XlKal0LE9JCrA8JSnA8pSkAMtTkgKKUJ51L5KfMmVKX14q5ebmdhVvra2tFTLQ3d1d\n05anLBaPVjo6OjI4TH2WL1/O4sWLk2Zoa5vJu+/uTpoB4Mknn+TBBx9MmmHKlMkcP34iaQaAxx9/\nnEceeSRphsbGcRRgUGLp0qUsXbo0aYaGhgaov3cqH3/8cU07jhgxIov7uyRP2yWVThFO2y1PSaVj\neWaopaUldYTCuO2221JHKIzp06enjlAYM2fOTB0hM5ZnhizP35o6dWrqCIVxxx13pI5QGJZntgZM\neUq6eliekhTguypJUoCTpyQFWJ6SFGB5SlKA5SlJAZanJAVYnpIU4FIlSQpw8pSkAMtTkgIsT0kK\nsDwlKcDylKQAr7ZLUoCTpyQFWJ6SFGB5SlKA5SlJAZanJAVYnpIU4FIlSQoowuQ5qIZ9ZgPvAweA\nR/ONI0m9q1QqNW156m3yHAz8HJgFHAd2ABuA93JNJUlXUIbJ83bgIHAE+AJYA8zNOZMkXVEZJs9G\n4NhFn38ETMsvTszevXt54403+nSb7du3c+jQIRYv/lvmzLmbUaNG5ZSuf+3cuZPt27f36Tavvvoq\nR48e4wc/+CELFsxj5MiROaXrX6+99hrbtm3r023a29s5deo0P/7xI9x//yJGjBiRUzrVowiTZ2/l\nWVPC5cuXX/i4paWFlpaWOiL13UsvbWTTpl+Fbrt69bM0NU2gtbU141RprFmzlldeeTl022XLVvC1\nrzVz5513ZpwqjV/84l/p7Nwcuu0TT/wLkyffwowZMzJOdXXp7Oyks7Mz8+MWoTwbevnzPwWWUr1o\nBPAY0A3880X7VDo6OrJPlrPTp09z6NAhWlpaGDZsWCbHbGubybvv7s7kWP3p5MmTHD58mKlTpzJ0\n6NBMjjllymSOHz+RybH609GjR/nwww+ZPn06Q4ZksxilsXEcBfi/XggNDQ3Qe+/0prJjx46adpw6\ndWoW93dJvX137ASagSbgBHAfsDCPIP1t9OjRjB49OnWMQhg7dixjx45NHaMQJkyYwIQJE1LHUC+K\nMHn2Vp7ngYeAl6leeV+GV9olJVaG8gTY2LNJUiGUpTwlqVAsT0kKsDwlKcDylKQA31VJkgKcPCUp\nwPKUpADLU5ICLE9JCrA8JSnA8pSkAJcqSVKAk6ckBViekhRgeUpSgOUpSQGWpyQFeLVdkgKcPCUp\nwPKUpADLU5ICLE9JCrA8JSnA8pSkAJcqSVKAk6ckBViekhRgeUpSwIApz3PnzmVxmAHhzJkzqSMU\nxgcffJA6QkGMo6GhIXWIAaUI5ZnFv2gF0v9FJJVBw4Vf6lBZtWpVTTsuWrQoi/u7pEwmzxdf/Pcs\nDlN699xzNx0dnaljFEJb20w2b+5IHaMQvvnNNnL6/3vVcqmSJAUU4bTd8pRUOpanJAVYnpIUYHlK\nUoDlKUkBlqckBbhUSZICnDwlKcDylKQAy1OSAixPSQqwPCUpwPKUpACXKklSQBEmz0GpA2Th2WfX\n09XVlTpGIaxc+Qxnz55NHaMQli9fwWeffZY6hnJQqVRq2vJU+vLs6upi5coVdHZuSR0luZMnT/L0\n08vYssXH4siRI6xcuYLXX389dRTlwPLMwI4dO4AK69Y9nzpKctu2bQNg7dr2xEnS+/KxeOGFjYmT\nKA+WZwamTZsGQFvbjMRJ0mtrawPgW9+6K3GS9O66q/oYzJkzO3ES5aEI5Vn6C0YjRowA4Kabbkqc\nJL1rr70W8LEAGDNmDABNTU1pgygXXm2XpIAiXG23PCWVjuUpSQGWpyQFWJ6SFGB5SlKA5SlJAUVY\nqlTLIvmngFPAnpyzSFJN6lwkPxt4HzgAPBrNUEt5Pt1zZ5JUCHWU52Dg51Q7bSKwEPh6JEMt5flr\nwLcsklQYdZTn7cBB4AjwBbAGmBvJUPrXtku6+tRRno3AsYs+/6jna32WyQWjVatWXfh40qRJTJo0\nKYvD1mzevPn9fp9FNW/efJqbm1PHKIR77pnLuHHjUsdQDi73fObhw4c5cuTIFW+aVYZMynPRokVZ\nHCZs8eK/SXr/RfLQQz9MHaEwHn54SeoIysnlyrOpqel33gzmEu9texwYf9Hn46lOn33mUiVJpVPH\nUqWdQDPQBJwA7qN60ajPannOczWwDfgq1ecKHojckSRlpY7nPM8DDwEvA/uBXwLvRTLUMnmGWlmS\n8lLnK4w29mx18bRdUun48kxJCrA8JSnA8pSkAMtTkgKK8K5Klqek0nHylKQAy1OSAixPSQqwPCUp\nwPKUpADLU5ICXKokSQFOnpIUYHlKUoDlKUkBlqckBViekhTg1XZJCnDylKQAy1OSAixPSQqwPCUp\nwPKUpADLU5ICXKokSQFOnpIUYHlKUoDlKUkBRSjPhgyOUYH0fxFJZdBw4Zc6VBYuXFjTjqtXr87i\n/i4pk8lz9eo1WRym9BYu/C7t7S+kjlEI9947l/Xrn0sdoxDmzfsOP/vZE6ljFMKSJdkcpwiTp6ft\nkkrHpUqSFODkKUkBlqckBViekhRgeUpSgOUpSQGWpyQFuFRJkgKcPCUpwPKUpADLU5ICLE9JCrA8\nJSnA8pSkgCIsVRqUOkAW1q5dR1dXV+oYhbBy5TOcPXs2dYzkKpUKK1b8G59++mnqKMpBpVKpactT\n6cvzzJkzPP/8erZufT11lOROnjzJc8+tY+vWramjJHf06FE2bGjnrbfeSh1FObA8M7Br1y4A2ttf\nTJwkvZ07dwKwYcPGxEnS27Gj+lhs2tSROInyYHlmYNq0aQDMmjUzbZACaG1tBeDuu/8icZL0Zs36\ncwDmzPGxGIiKUJ6lv2A0fPhwAG644YbESdK75pprAGhsbEycJL2RI0cCcP311ydOojx4tV2SAopw\ntd3ylFQ6Tp6SFGB5SlKA5SlJAZanJAVYnpIUUITyrGWR/HigA9gH7AV+lGsiSepFd3d3TVueapk8\nvwAeBt4BhgNvA68A7+WYS5IuqwiTZy3l+ZueDeAc1dIch+UpKZGylOfFmoBbgTezjyLpcj7//HM2\nb97Cpk2/olLpZvLkyX26/Y033khbWxsNDQ05JexfZSvP4cA6YAnVCfSCdevWXfh44sSJTJw4MZNw\ntZo/fwG33HJLv95nUS1Y8FfcfPPNqWMUwty59zJ27NjUMTLxySef8Pbb+6hUqs/j7d69u0+3P3Xq\nP2ltbWXw4MF5xLusAwcOcPDgwcyPW6byHAqsB54B2n//D+fPn59lpj6bN+87Se+/SL73vUWpIxTG\n97//16kjZOa6667jJz/5O06fPk2lUmHMmDGpI9WkubmZ5ubmC59v2rQpk+OWpTwbgGXAfuCn+caR\ndCWjR49OHaEQylKefwbcD+wGdvV87TEgmx8hktRHZXlXpa0MgDdNljRwlGXylKRCsTwlKcDylKQA\ny1OSAixPSQqwPCUpoCxLlSSpUJw8JSnA8pSkAMtTkgIsT0kKsDwlKcDylKQAlypJUoCTpyQFWJ6S\nFGB5SlKA5SlJAZanJAVYnpIU4FIlSQpw8pSkAMtTkgIsT0kKsDwlKaAI5TkodYCs7N+/P3WEwtiz\nZ0/qCIWxd+/e1BEK48CBA6kjZKa7u7umLU+W5wBkYfzWvn37UkcojIMHD6aOkJlKpVLTlidP2yWV\nThFO2y1PSaVThPJsyOAYnUBrBseRNPBtAWbWeYzK8OHDa9rx3LlzkE3P/T9ZTJ4zMziGJNWsCJOn\np+2SSsfylKSAIrwxyEBYqjQbeB84ADyaOEtKTwGnABd5wnigA9gH7AV+lDZOUn8AvAm8A+wH/ilt\nnGwUYalS2Q0GDgJNwFCq3yBfTxkooRnArVieANcDLT0fDwc+4Or9vgD4w57fhwBvAN9ImCULlWHD\nhtW0Abk1aNknz9uplucR4AtgDTA3ZaCEfg10pQ5REL+h+oMU4BzwHjAuXZzkPuv5fRjVgeNMwiyZ\nyGnyXED1bOV/gT/ubeeyl2cjcOyizz/q+Zr0pSaqE/mbiXOkNIjqD5NTVJ/OKP3L8XIqzz3At4HX\natm57BeMfFJDVzIcWAcsoTqBXq26qT6NMQJ4meryws6EeeqW0/OZ7/dl57JPnsepXhz40niq06c0\nFFgPPAO0J85SFB8DLwG3pQ5SryJcMCr75LkTaKZ6anYCuA9YmDKQCqEBWEb19PSnibOkNho4D/wX\n8BXgLuAfkybKQB1LlV6hekHx9/0D8GI4UEn9JdWrqQeBxxJnSWk11R8g/0P1eeAH0sZJ6htUT1Xf\nAXb1bLOTJkpnEvAfVB+L3cDfp42TiUoftrOB43dQwwUjSdLv6gD+JHUISSqLb1M9a/tvqsvdNqaN\nI0mSJEmSJEmSJEmSJEmSJElSP/k/Kx1shyi1yzQAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f5b9c6b0090>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Discovered Policy and Values\n",
      "----\n",
      "***\n",
      "States/Rewards       | Actions  \n",
      "-------------        | --------\n",
      "<img src=\"grid_solved.png\"> | left, right, up, down\n",
      "\n",
      "<p style='text-align: center;'>**Transitions**</p>\n",
      "<p style='text-align: center;'>$T(s\\_action|\\cdot, s\\_action) = 0.8 \\\\ T(s\\_left|\\cdot,s\\_action) = 0.1\\\\ T(s\\_right|\\cdot,s\\_action) = 0.1 $</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Free Methods\n",
      "---\n",
      "***\n",
      "* We have assumed the agent has a copy of the MDP (a *model*) in its head\n",
      "* This is rarely the case\n",
      "* Generaly the agent can only interact with the envinroment\n",
      "* i.e., the agent is imbued in a statistical bath of states, actions, trantisions and rewards\n",
      "* An agent samples its enviroment until it reached a terminal state\n",
      "    * Tuples in the form (s',a',r')\n",
      "    * [(1,0), left, -0.04], [(2,0), left, -0.04],[(3,0), left, -0.04], [(3,1), up, -1]\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (1)\n",
      "---\n",
      "***\n",
      "* Remember Q is just E[\n",
      "* MC (Naive Version)\n",
      "    * Start at any state, initialise $Q_0(s,a)$ as you visit states/actions \n",
      "    * Act $\\epsilon$-greedily\n",
      "        * $\\pi^\\epsilon(s,a) = \\begin{cases} 1-\\epsilon + \\epsilon/|A| , & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q_n(s,a)$} \\\\ \\epsilon/|A|, & \\mbox{if } \\mbox{ otherwise} \\end{cases}$\n",
      "    * Wait until episode ends\n",
      "    * Add all reward you have seen so far to $\\mathrm{v_\\tau^i}  = R(s')+ \\gamma R(s'') + \\gamma^2 R(s''') + \\gamma^{\\tau-1}R(s^\\tau)$ for episode $i$\n",
      "    * $Q_n(s,a) =  E_{\\pi^\\epsilon}[\\mathrm{v_\\tau^i}]  = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\mathrm{v_\\tau^i}} $, where $k$ is the times a state is visited  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (2)\n",
      "---\n",
      "***\n",
      "* $\\epsilon$-greedy means acting greedily $1-\\epsilon$, random otherwise\n",
      "* Better to calculate mean incrementaly\n",
      "\\begin{align*}\n",
      "Q_n(s,a) &= E_{\\pi_n}[\\mathrm{v_\\tau^i}]\\\\\n",
      "Q_n(s,a) &= \\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\mathrm{v_\\tau^i}}\\\\\n",
      "Q_n(s,a) &= \\frac{1}{n}\\left(\\mathrm{v_t^1} + \\mathrm{v_\\tau^2}....\\mathrm{v_\\tau^{n-1}} +  \\mathrm{v_\\tau^n}\\right)\\\\\n",
      "Q_n(s,a) &= \\frac{1}{n}\\left(\\sum\\limits_{i=1}^{ n-1}{\\mathrm{v_\\tau^i}} +  \\mathrm{v_\\tau^n}\\right)\\\\\n",
      "\\end{align*}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (3)\n",
      "---\n",
      "***\n",
      "by definition $Q_{n-1}(s,a) = \\frac{1}{{n-1}}\\sum\\limits_{i=1}^{n-1}{\\mathrm{v_\\tau^i}} \\implies (n-1)Q_{n-1}(s,a) =  \\sum\\limits_{i=1}^{n-1}{\\mathrm{v_\\tau^i}} $\n",
      "\\begin{align*}\n",
      "Q_n(s,a) &= \\frac{1}{n}\\left((n-1)Q_{n-1}(s,a) +  \\mathrm{v_\\tau^n}\\right) \\\\\n",
      "Q_n(s,a) &= \\frac{1}{n}\\left(Q_{n-1}(s,a)k - Q_{n-1}(s,a) + \\mathrm{v_\\tau^n}\\right)\\\\\n",
      "Q_n(s,a) &= \\frac{Q_{n-1}(s,a)k}{n} + \\frac{-Q_{n-1}(s,a) + \\mathrm{v_\\tau^n}}{n}\\\\\n",
      "Q_n(s,a) &= Q_{n-1}(s,a) + \\frac{\\overbrace{\\mathrm{v_\\tau^n} - Q_{n-1}(s,a)}^{\\textbf{MC-Error}} }{n}\\\\\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control - Putting it all togeather\n",
      "---\n",
      "***\n",
      "* But $\\pi^n$ changes continuously, so the distribution of rewards is non-stationary\n",
      "\\begin{align*}\n",
      "Q_n(s,a) &= Q_{n-1}(s,a) + \\frac{1}{n}\\left[\\mathrm{v_\\tau^n} - Q_{n-1}(s,a) \\right] \\rightarrow \\textbf{Bandit case} \\\\\n",
      "Q_n(s,a) &= Q_{n-1}(s,a) + \\alpha\\left[\\mathrm{v_\\tau^n} - Q_{n-1}(s,a) \\right]  \\rightarrow \\textbf{Full MDP case}\\\\\n",
      "\\end{align*}\n",
      "* A Bandit is an MDP with a chain of length two (i.e. s, s') - like the initial EagleWorld \n",
      "* $\\alpha$ is a learning rate, usually set to a small value (e.g., 0.001)\n",
      "\n",
      "* MC\n",
      "    * Start at any state, initialise $Q_0(s,a)$ as you visit states/actions \n",
      "    * Act $\\epsilon$-greedily\n",
      "        * $\\pi_\\epsilon^n(s,a) = \\begin{cases} 1-\\epsilon + \\epsilon/|A| , & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q_n(s,a)$} \\\\ \\epsilon/|A|, & \\mbox{if } \\mbox{ otherwise} \\end{cases}$\n",
      "    * Wait until episode ends, i.e. a terminal state is hit - $\\epsilon$ set to some low value, e.g., 0.1\n",
      "    * Add all reward you have seen so far to $\\mathrm{v_\\tau^i}  = R(s)+\\gamma R(s')+...\\gamma^2R(s'') + \\gamma^{\\tau-1}R(s^\\tau)$ for episode $i$\n",
      "    * $Q_n(s,a) = Q_{n-1}(s,a) + \\alpha\\left[\\mathrm{v_\\tau^n} - Q_{n-1}(s,a) \\right]$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simplified BlackJackWorld\n",
      "---\n",
      "***\n",
      "* We are going to use a simplified version of BlackJack, a popular card game, as an example\n",
      "* Though it is possible to create a model, it's not that trivial, which makes it ideal for model-free methods\n",
      "* Game Rules as follows\n",
      "    * All cards have their face values, except J,Q,K that have a value of 10 and A that has a value of 11\n",
      "    * Goal of the game is to reach 21 - or if you have two aces 22 (that's a deviation from the original game)\n",
      "    * A player and a Dealer are dealt 2 cards each - the dealer keeps the second card hidden\n",
      "    * A player can \"stick\" - no more cards received or \"hit\" (get more cards)\n",
      "        * If the score of 21 (or 22 in case of two aces) is passed, the player loses (reward -1)\n",
      "        * If the player has a higher score than the dealer, she wins (reward +1)\n",
      "        * If scores are equal (reward 0)\n",
      "    * Dealer has a fixed policy, if score less than 17, she has to draw more cards"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from blackjack import SimpleBlackJack\n",
      "import utils\n",
      "\n",
      "\n",
      "class RLAgent():\n",
      "    \"\"\"A SARSA/MC Agent\n",
      "    \"\"\"\n",
      "    def __init__(self,learning_rate = 0.01, epsilon = 0.01):\n",
      "        self.Q = {}\n",
      "        self.learning_rate = learning_rate\n",
      "        self.epsilon = epsilon\n",
      "\n",
      "    def MC(self,visited_states, actions, rewards):\n",
      "        \"\"\"\" Monte Carlo Control \"\"\"\n",
      "        v_t = sum(rewards)\n",
      "\n",
      "        for i in range(len(visited_states)):\n",
      "            state = visited_states[i]\n",
      "            action = actions[i]\n",
      "            #if(state[0] == 4 and action == \"stick\"):\n",
      "            #    print state, action, visited_states,actions, rewards\n",
      "            state_action, Q = self.__Q(state, action)\n",
      "            self.Q[state_action]+= self.learning_rate*(v_t - self.Q[state_action] )\n",
      "\n",
      "    def SARSA(self, previous_state, previous_action, state, action, next_reward):\n",
      "        \"\"\"\" SARSA(0) \"\"\"\n",
      "        if(state is None):\n",
      "            Q = 0\n",
      "        else:\n",
      "            state_action, Q = self.__Q(state, action)\n",
      "        previous_state_action, previous_Q = self.__Q(previous_state, previous_action)\n",
      "        #self.Q[state_action]+= self.learning_rate*(reward + previous_Q - self.Q[state_action] )\n",
      "        self.Q[previous_state_action]+= self.learning_rate*(next_reward + Q - self.Q[previous_state_action]  )\n",
      "\n",
      "\n",
      "    def selectAction(self, state, actions):\n",
      "        \"\"\" Select an action given a state and a vector of possible actions\"\"\"\n",
      "        r = random.random()\n",
      "        if(len(actions) == 1):\n",
      "            return actions[0]\n",
      "        if(r < self.epsilon):\n",
      "            return random.choice(actions)\n",
      "        else:\n",
      "\n",
      "            return utils.argmax(actions, lambda action: self.__Q(state,action)[1])\n",
      "\n",
      "    def __Q(self,state, action):\n",
      "        if(state is None or action is None):\n",
      "            return 0\n",
      "        state_action = tuple(state + [action])\n",
      "\n",
      "        if(state_action not in self.Q):\n",
      "            self.Q[state_action] = 0\n",
      "\n",
      "        return state_action, self.Q[state_action]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blackjack = SimpleBlackJack()\n",
      "agent = RLAgent(learning_rate=0.01, epsilon=0.01)\n",
      "blackjack.simulate(agent,200000,episodic=True)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vis = Visualiser()\n",
      "vis.Q(agent.Q, \"RL-MC.png\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAADyCAYAAADnXrZCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f5b9c6bb2d0>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Blackjack MC Results\n",
      "---\n",
      "***\n",
      "\n",
      "<img src=\"RL-MC.png\">\n",
      "* 16 Onwards \"stick\", less than that \"hit\"\n",
      "* Is this correct - looks like it is\n",
      "* Can you think of an adequete test ? \n",
      "* What if there are not terminal states ? \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "SARSA(0)\n",
      "---\n",
      "***\n",
      "* Stands for State-Action-Reward-State-Action\n",
      "* 0 stands for a parameter called $\\lambda$, which we are going to ignore here\n",
      "* The MC update rule \n",
      "    * $Q_n(s,a) = Q_{n-1}(s,a) + \\alpha\\left[\\mathrm{v_\\tau^n} - Q_{n-1}(s,a) \\right]$\n",
      "* Let's try to create an incremental version of MC\n",
      "\n",
      "\\begin{align*}\n",
      "\\mathrm{v_\\tau^i}  &= R(s')+ \\gamma R(s'') + \\gamma^2 R(s''') + \\gamma^{\\tau-1}R(s^\\tau) \\\\\n",
      "\\mathrm{v_\\tau^n} &=  R(s') + \\gamma \\left(R(s'') + \\gamma R(s''') + \\gamma^{\\tau-1}R(s^\\tau)\\right)  \\\\\n",
      "\\mathrm{v_\\tau^n} &=  R(s') + \\gamma \\mathrm{v_\\tau^{n '}} \\\\\n",
      "E_{\\pi_n}[\\mathrm{v_\\tau^i}] &= E_{\\pi_n}[R(s') + \\gamma \\mathrm{v_\\tau^{n '}}]\\\\\n",
      "E_{\\pi_n}[\\mathrm{v_\\tau^i}] &= E_{\\pi_n}[R(s')] + E_{\\pi_n}[\\gamma \\mathrm{v_\\tau^{n '}}]\\\\\n",
      "E_{\\pi_n}[\\mathrm{v_\\tau^i}] &= E_{\\pi_n}[R(s') + \\gamma Q(s',a')]\\\\\n",
      "\\mathrm{v_\\tau^n} &=  R(s') + \\gamma Q(s',a') \n",
      "\\end{align*}\n",
      "\n",
      "    \n",
      "* $Q_n(s,a) = Q_{n-1}(s,a) + \\alpha\\left[R(s') + \\gamma Q_{n-1}(s',a') - Q_{n-1}(s,a) \\right]$\n",
      "    \n",
      "* Dynamic Programming targets are closer to SARSA rathen than MC\n",
      "    \n",
      "    \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blackjack = SimpleBlackJack()\n",
      "agent = RLAgent(learning_rate=0.01, epsilon=0.01)\n",
      "blackjack.simulate(agent,200000,episodic=False)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vis = Visualiser()\n",
      "vis.Q(agent.Q, \"RL-SARSA.png\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAADyCAYAAADnXrZCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f5b9c6bb3d0>"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Blackjack SARSA Results\n",
      "---\n",
      "***\n",
      "MC       | SARSA  \n",
      "-------------        | --------\n",
      "<img src=\"RL-MC.png\"> | <img src=\"RL-SARSA.png\"> \n",
      "\n",
      "\n",
      "* 16 Onwards \"stick\", less than that \"hit\"\n",
      "* Similiar to MC\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MC vs SARSA\n",
      "---\n",
      "***\n",
      "* Not exactly the same\n",
      "* SARSA(0) has lower variance\n",
      "* Requires stronger Markov guarantees\n",
      "* MC works even in environments with sensor aliasing/partially observability\n",
      "* MC slower to converge, higher variance\n",
      "* In very long chains, MC should be prefered, especially if rewards are very delayed, as initial states may not see any rewards fast enough"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Conclusion\n",
      "---\n",
      "***\n",
      "* RL is a massive topic\n",
      "* We have shown the tip of iceberg\n",
      "* Rabbit hole goes *deep* - both on the application level and the theory level\n",
      "* Essex CS Department has a PhD programme associated with RL \n",
      "    * IGGI Centre\n",
      "    * Four year long PhD programme\n",
      "    * Chance to use RL in Computer Games\n",
      "    \n",
      "    \n",
      "        \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "References\n",
      "---\n",
      "***\n",
      "* David Silver's UCL Course: http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html\n",
      "    * Some ideas in these lecture notes taken from there\n",
      "    * Probably the best set of notes there is on the subject\n",
      "    * Lectures 1,2,3,4 \n",
      "* Reinforcement Learning, by Richard S. Sutton and Andrew G. Barto \n",
      "    * Classic book\n",
      "    * Excellent treatment of most subjects\n",
      "    * Up to Chapter 5\n",
      "* [AIM],Artificial Intelligence: A Modern Approach by Stuart J. Russell and Peter Norvig\n",
      "    * The Introductory A.I. Textbook\n",
      "    * Chapters 16 and 21\n",
      "    * Examples used here from this book, but different terminology\n",
      "* Algorithms for Reinfocement Learning by Csaba Szepesvari\n",
      "    * Very \"Mathematical\", but a good resource that provides a very unified view of the field\n",
      "* Reinforcement Learning: State-Of-The-Art by Marco Wiering (Editor), Martijn Van Otterlo (Editor) \n",
      "    * Edited Volume\n",
      "    * Chapter 1\n",
      "        \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Part Two Review Questions\n",
      "---\n",
      "***\n",
      "* How is a Q-Value related to the mean rewards ?\n",
      "* What is relationship between V and Q Values ?\n",
      "* Write down an example episode state/action/reward values for Gridworld\n",
      "    a. Calculate the top node V and Q using bellman backups\n",
      "    b. Modify sample Q-Values starting with initial values of 0, $\\alpha$ \n",
      "* Do value iteration for the first 3 steps in EagleWorld MDP\n",
      "* Perform the first 2 steps of policy iteration in EagleWorld MDP, for K = 2\n",
      "* Run two iterations of SARSA in EagleWorld MDP\n",
      "* Run two iterations of MC in EagleWorld MDP\n",
      "* You run MC in uknown enviroment and you find out that each chain as 50 steps, all containing different states. You have a very limited budget of iterations $k=30$. Would you switch to SARSA why ?\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}