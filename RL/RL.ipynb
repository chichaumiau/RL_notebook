{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:80caa6f727d49e5619d3bdd94043ec6544892ecaf18893589b841bd66f1c12f1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Reinforcement Learning\n",
      "===="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Outline\n",
      "---\n",
      "***\n",
      "* Part I Introduction\n",
      "    * Introduction & Motivation\n",
      "        * What is Reinforcement Learning (RL) ?\n",
      "        * Markov Decision Process (MDPs)\n",
      "    * Model-Based Reinforcement Learning\n",
      "        * Bellman Backups\n",
      "* Part II Iterative Methods\n",
      "    * Model-Based Reinforcement Learning\n",
      "        * Value Iteration \n",
      "        * Policy Iteration \n",
      "    * Model-Free Reinforcement Learning\n",
      "        * Monte Carlo Control\n",
      "        * SARSA\n",
      "        * Q-Learning\n",
      "* Part III \n",
      "    * Discussion, References, Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is Reinforcment Learning? \n",
      "---\n",
      "***\n",
      "* *Reinforcement learning is the study of how animals and artificial systems can learn to optimize their behavior in the face of rewards and punishments* -- Peter Dyan, Encyclopedia of Cognitive Science\n",
      "* Not Supervised Learning - the animal/agent is not provided with examples of optimal behaviour, it has to be discovered!\n",
      "* It subsumes most Artificial Intelligence problems. Forms the basis of most modern intelligent agents frameworks\n",
      "* Ideas drawn from a wide range of contexts, including psychology (e.g, Skinner's \"Operant Conditioning\") and Philosophy (e.g, \"Freedom, Equality, Property and Bentham\"), neuroscience, operations research  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* Play Backgammon/Chess/Go/Poker (at human or superhuman level)\n",
      "* Hellicopter Control\n",
      "* Learn how to walk/crawl/swim/cycle\n",
      "* Elevator Scheduling\n",
      "* Optimising an petreulem refinery \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Decision Process\n",
      "---\n",
      "***\n",
      "* The primary abstraction we are going to work in is the Markov Decision Process (MDP). \n",
      "* MDPs capture the dynamics of a mini-world/universe/environment\n",
      "* An MDP is defined as a tuple $<S,A,T,R,\\gamma>$ where: \n",
      "    * $S$, $s \\in S$ is a set of states\n",
      "    * $A$, $a \\in A$ is a set of actions\n",
      "    * $R:S$, $R(s)$ is a function that maps states to rewards\n",
      "    * $T:S\\times S\\times A$, with $T(s'| s, a)$ being the probability of an agent landing from state $s$ to state $s'$ if it takes action $a$\n",
      "    * $\\gamma$ is a discount factor - the impact of time on rewards\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 615,AIM]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.99):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state]\n",
      "\n",
      "    def T(state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (result-state, probability) pairs.\"\"\"\n",
      "        abstract\n",
      "        \n",
      "    def sample(state, action):\n",
      "        \"\"\"Sample from state action. Returns a new state\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov Property and States\n",
      "---\n",
      "***\n",
      "* States represent sufficient statistics. \n",
      "* Markov Property ensures that we only care about the present in order to act, not past states\n",
      "* Think Tetris - All information are can be captured by a single screenshot\n",
      "\n",
      "1984 Original Version|1986 DOS Version \n",
      "-------------        | -------------\n",
      "<img src=\"250px-Tetris-VeryFirstVersion.png\" alt=\"Drawing\" style=\"width: 300px;\">           | <img src=\"250px-Tetris_DOS_1986.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
      "* States $s$ where there are no actions are called *terminal*(e.g., endgames) - there are other definitions as well\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents, Actions and Transitions\n",
      "---\n",
      "***\n",
      "* An agent is an entity capable of actions\n",
      "* An MDP can capture any environment that is inhabited either by exaclty one agent or that other agents are not adaptive\n",
      "* Notice how actions are described by the MDP, which captures the world dynamics, not the agent\n",
      "* In effect, the agent is just a \"brain in a vat\", an action-selector\n",
      "* The Agent perceives states/rewards and outputs actions\n",
      "* Transitions specify the effects of actions in the world (e.g.,  in Tetris, you push a button, the block spings)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Rewards and the Discount Factor\n",
      "---\n",
      "***\n",
      "* Rewards describe state preferences\n",
      "* Agent is happier in some states of the MDP (e.g. in Tetris when the block level is low - a fish in water)\n",
      "* Punishment is just low/negative reward\n",
      "* $\\gamma$\n",
      "    * the discount factor, describes the impact of time on rewards \n",
      "    * \"I want it now\", the lower $\\gamma$ is the less important future rewards are \n",
      "* There are no \"springs of rewards\" in the real world - \"human nature ?\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Examples of Reward Schemes\n",
      "---\n",
      "***\n",
      "* Scoring in most video games\n",
      "* The distance a robot walked for a bipedal robot\n",
      "* The amount of food an animal eats\n",
      "* Money in modern societies\n",
      "* Army Medals (\"Gamification\") \n",
      "*  (- Fuel spend on a flight)(+ Distance Covered)\n",
      "* Cold/Hot "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Long Term Thinking\n",
      "---\n",
      "* It might be better to delay satisfaction\n",
      "* Immidiate reward is not allways the maximum reward\n",
      "* In some settings there are no immidiate rewards at all (e.g. some solitaire games) \n",
      "* MDPs and RL capture this\n",
      "* \"Not going out tonight, study\"\n",
      "* Long term investement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Policy\n",
      "---\n",
      "***\n",
      "* The MDP (the universe) is populated by an Agent (an actor)\n",
      "* You can take actions (e.g. move around, move blocks)\n",
      "* The type of actions you take under a state is called the *policy*\n",
      "* $\\pi: S \\times A$, $\\pi(s,a) = P(s|a)$, a probabalistic mapping between states and actions\n",
      "* Finding an optimal policy is *mostly* what the RL problem is all about\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Full Loop\n",
      "---\n",
      "***\n",
      "* See how the universe described by the MDP defines actions, not just states and transitions\n",
      "* An agent needs to action upon what it perceives\n",
      "* Notice the lack of body - \"brain in a vat\". Body is assumed to be part of the world. \n",
      "\n",
      "<img style=\"float:centre\" src=\"RL.png\">\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld\n",
      "---\n",
      "***\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agent Goals\n",
      "---\n",
      "***\n",
      "* The agents goal is to maximise its long term reward $ V^\\pi(s) =  \\mathbb{E}_{\\pi}\\left[\\sum\\limits_{t=0}^\\infty{\\gamma^tR \\left( s^{t} \\right) }\\right]$\n",
      "\n",
      "* $\\pi^* = \\underset{\\pi}{\\operatorname{argmax}}  V^\\pi(s_0)$\n",
      "* Risk Neutral Agent - think of the EagleWorld Example\n",
      "* Rewards can be anything, but most organisms receive rewards only in a very limited amount of states (e.g., fish in water)\n",
      "* What if your reward signal is money ?\n",
      "* Sociopathic, Egotistic, Greed-is-good Gordon Gecco\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Searching for a Good Policy\n",
      "---\n",
      "***\n",
      "* One can possibly search through all combinations of policies until he finds the best\n",
      "* Slow, does not work in larger MDPs\n",
      "* Exploration/Exploitation Dillema\n",
      "    * How much time/effort should be spend exploring for solutions\n",
      "    * How much time should be spend exploiting good solutions\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Based Reinforcement Learning\n",
      "---\n",
      "***\n",
      "* ...also known as planning in certain contexts\n",
      "* Who was doing the thinking in the previous example (You? The eagle ?)  \n",
      "* An agent has access to model, i.e. has a copy of the MDP (the outside world) in its brain\n",
      "* Using that copy, it tries to \"think\" what is the best route of action\n",
      "* It than executes this policy on the real world MDP\n",
      "* You can't really copy the world inside your head, but you can copy the dynamics\n",
      "* \"This and that will happen if I push the chair\"\n",
      "* Thinking, introspection..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bellman Expectation Equations / Bellman Backups\n",
      "---\n",
      "***\n",
      "* There are the two most important functions related to an MDP \n",
      "* $V(s) = \\mathbb{E}_{\\pi}\\left[\\sum\\limits_{t=0}^\\infty{\\gamma^tR \\left( s^{t} \\right) }\\right]$\n",
      "* Recursive definitions\n",
      "* $ {V^\\pi (s) = R(s) + \\gamma\\sum\\limits_{a \\in A}\\pi(s,a)\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')     \\right)}$ \n",
      "* ${Q^\\pi (s,a) =  \\sum\\limits_{s' \\in S} T(s'|s,a)\\left(R(s') +   \\gamma\\sum\\limits_{a' \\in A}\\pi(a',s')Q(s',a')    \\right)}$ \n",
      "* Called  V-Value(s) (*state-value function*) and Q-Value(s) (*state-action value function*) respectively\n",
      "* They are also interrelated\n",
      "* $Q^\\pi(s,a) = \\sum\\limits_{s' \\in S} T(s'|s,a) V^\\pi(s')$\n",
      "* $V^\\pi(s) =  R(s) + \\gamma\\sum\\limits_{a' \\in A} \\pi(s,a) Q^\\pi(s,a)$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Optimal Policy and the Bellman Optimality Equation\n",
      "---\n",
      "***\n",
      "* An optimal policy can be defined in terms of Q-values\n",
      "* It is the policy that maximises Q values\n",
      "* ${Q^* (s,a) =  \\sum\\limits_{s' \\in S} T(s'|s,a)\\left(R(s') +   \\gamma\\max\\limits_{a' \\in A}\\pi(a',s')Q(s',a')    \\right)}$ \n",
      "* $ {V^* (s) = R(s) + \\gamma\\max\\limits_{a \\in A}\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V^*(s')     \\right)}$ \n",
      "* $\\pi^*(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q^*(s,a)$} \\\\ 0, &  \\mbox{ otherwise} \\end{cases}$\n",
      "* $V^*(s) = R(s) +  \\gamma\\sum\\limits_{a' \\in A} \\pi^*(s,a) Q^{*}(s,a) = R(s) + \\gamma\\max\\limits_{a' \\in A}  Q^{*}(s,a)$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Q_value(a, s, V, mdp):\n",
      "    \"The expected reward of doing action a in state s, according to the MDP and V.\"\n",
      "    return sum([p * V[s_prime] for (p, s_prime) in mdp.T(s, a)])\n",
      "\n",
      "def optimal_policy(mdp, V):\n",
      "    \"\"\"Given an MDP and a value function V, determine the best policy,\n",
      "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
      "    pi = {}\n",
      "    for s in mdp.states:\n",
      "        pi[s] = argmax(mdp.actions(s), lambda a:Q_value(a, s, V, mdp))\n",
      "    return pi\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Random Policy\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n",
      "<center><font color='red'>$\\pi(Flying, Attack\\_Boar) = 0.5, \\pi(Flying, Attack\\_Turtle) = 0.5$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$V^\\pi(Flying) = 0.5  Q^\\pi(Flying, Attack\\_Turtle) + 0.5  Q(Flying, Attack\\_Boar) = 3.5145 $</font></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Optimal Policy\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample-simple.png\">\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$\\pi^*(Flying, Attack\\_Boar) = 1$, $\\pi^*(Flying, Attack\\_Turtle) = 0$ </font></center>\n",
      "<center><font color='red'>$V^*(Flying) = 0.0  Q^\\pi(Flying) + 1.0  Q(Flying, Attack\\_Boar) = 4.455 $</font></center>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agents Revisited\n",
      "---\n",
      "***\n",
      "* An Agent can be composed of a number of things\n",
      "  * A policy \n",
      "  * A Q-Value/and or V-Value Function\n",
      "  * A Model of the environment (the MDP)\n",
      "  * Inference/Learning Mechanisms\n",
      "  * ...\n",
      "* An agent has to be able to *create a policy* either on the fly or using Q-Values \n",
      "* The Model/Q/V-Values serve as intermediate points towards construcing a policy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Relationship to the rest of Machine Learning\n",
      "---\n",
      "***\n",
      "* How can one learn a model of the world ? \n",
      "    * Possibly by breaking it down into smaller, abstract chunks\n",
      "        * Unsupervised Learning\n",
      "    * ... and learning what effects ones actions have the envinroment\n",
      "        * Supervised Learning\n",
      "* RL weaves all fields of Machine Learning (and possibly Artificial Intelligence) into one coherent whole\n",
      "* The purpose of all learning is action!\n",
      "    * You need to be able to recognise faces so you can create state\n",
      "    * ... and act on it\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "*Review Questions*\n",
      "---\n",
      "***\n",
      "1. Give a brief, intuitive description of Reinforcment Learning\n",
      "2. Define a Markov Decision Process\n",
      "2. Define an Agent\n",
      "3. Define the policy of an agent\n",
      "5. What is an Optimal Policy ? \n",
      "4. Assume an MDP with with single state $s_0$ and two actions $a_0, a_1$. Assume a policy $\\pi(s_0|a_0) = 0.3$, $\\pi(s_0|a_1) = 0.7$. Briefly give an intuitive explanation of the policy in this setting. \n",
      "5. Assume an MDP with states $s_0, s_1, s_2$, $R(s_0) = 0$, $R(s_1) = 2$ and $R(s_2) = 1$ and actions $a_0,a_1$. Also assume transitions $T(s_1|s_0, a_0) = 1$, $T(s_2|s_0,a_1) = 1$. State $s_1,s_2$ are terminal  \n",
      "    1. What is $Q(s_0,a_0)$ and $Q(s_0,a_1)$ ? \n",
      "    1. What should be the policy $\\pi(s_0|a_0)$ and $\\pi(s_0|a_1)$ ? \n",
      "    1. What is $V(s_0)$ ? \n",
      "6. Think of a simple MDP and draw it. Does it have terminal states ? Can it be solved directly using Bellman Backups ? \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example MDP - EagleWorld - Recursion\n",
      "---\n",
      "***\n",
      "\n",
      "<img style=\"float:left\" src=\"MDPExample.png\">\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Boar)   = 0.99 * (10 * 0.5 + 0.5* -1) = 4.455$</font></center>\n",
      "<center><font color='red'>$Q(Flying, Attack\\_Turtle) = 0.99 * (0.9 * 3 + 0.1* -1) = 2.574$</font></center>\n",
      "<center><font color='red'>$Q^*(Flying, Keep\\_Flying) = 0.99  (max ( Q^*(Flying, Attack\\_Turtle) , Q(Flying, Attack\\_Boar), Q(Flying, Keep\\_Flying))$</font></center>\n",
      "<center><font color='red'>$\\pi(Flying,Attack\\_Boar = 1)$</font></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GridWorld (1)\n",
      "---\n",
      "***\n",
      "* As we saw previously, when they are cycles in the MDP (which might happen quite often) it's not easy to calculate the Bellman backups directly\n",
      "* An alternative is to used a method called *Policy Iteration*\n",
      "* Let's imagine an MDP full of loops - GridWorld\n",
      "    * A simple world, partitioned in a grid\n",
      "    * Goal it to get to the upper right corner, and avoid the block right below it (both terminal states)\n",
      "    * Rewards -0.04 everywhere, except the two terminal states (+1, -1) respectively\n",
      "    * MDP Transtions as follow\n",
      "        * $T(s\\_action|\\cdot, s\\_action) = 0.8 $\n",
      "        * $T(s\\_left|\\cdot,s\\_action) = 0.1 $\n",
      "        * $T(s\\_right|\\cdot,s\\_action) = 0.1 $\n",
      " \n",
      "    \n",
      "\n",
      "    \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GridWorld (2)\n",
      "---\n",
      "***\n",
      "States/Rewards       | Actions  \n",
      "-------------        | --------\n",
      "<img src=\"grid.png\"> | left, right, up, down\n",
      "\n",
      "<p style='text-align: center;'>**Transitions**</p>\n",
      "<p style='text-align: center;'>$T(s\\_action|\\cdot, s\\_action) = 0.8 \\\\ T(s\\_left|\\cdot,s\\_action) = 0.1\\\\ T(s\\_right|\\cdot,s\\_action) = 0.1 $</p>\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import *\n",
      "\n",
      "\n",
      "\n",
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.rows):\n",
      "            for y in range(self.cols):\n",
      "                self.reward[x, y] = grid[x][y]\n",
      "                if grid[x][y] is not None:\n",
      "                    self.states.add((x, y))\n",
      "        self.orig_grid = grid\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action == None:\n",
      "            return [(0.0, state)]\n",
      "        else:\n",
      "            return [(0.8, self.go(state, action)),\n",
      "                    (0.1, self.go(state, turn_right(action))),\n",
      "                    (0.1, self.go(state, turn_left(action)))]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = [[-0.04, -0.04, -0.04, +1],\n",
      "        [-0.04, -0.04, -0.04, -1],\n",
      "        [-0.04, -0.04, -0.04, -0.04]]\n",
      "terminals = [(0, 3), (1, 3)]\n",
      "gmdp = GridMDP(grid,terminals=terminals)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt(grid, fig = \"grid.png\")\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACZ9JREFUeJzt3E+IXfUZx+HvjDG0JaUIXRRl/mwULBRqW9zUIbNoIV2J\niyKRbtyL0kWRySrZZLYu3Go3YiwoSEux4iKTTpBKbRWjSYoBk4wRs2laCS3FdG4Xd5yJNsncvPfc\nnHMmzwM3mTvenHlzEj/5nXt+SQIAAAAAAAAAAABAg55PcjHJibYHAeiThSQPRDwBbtp8Gojn9Phz\nANx+xBOgQDyBXrnrrrsGSUZ9fDapOXZN6sAAk3Dp0qWsr6+P9Nrp6elvTmoOK0+gdwaDwUiPaziS\n5M0k9yVZS/J4dYap6g8EaMngypUrI71w165dyYQ657Id6J1RL9snSTyB3rnOJfktJZ5A74gnQIF4\nAhSIJ0CBeAIU7Ih4zszMDNbW1pqYBdjh9u7dm2PHjo2973JHbFVaW1vL0tJSE7OMZXV1NQsLC63O\nsLx8OEtLB1qdIXEurtaVc9GBhVIOHjyYgwcPtjrD1FQz+9V3xMoT4FYTT4AC8WzQ7Oxs2yN0hnOx\nxbnYsri42PYIjRHPBs3NzbU9Qmc4F1uciy3i2awdE0/g9iGeAAU7YqsSwK1m5QlQIJ4ABeIJUCCe\nAAXiCVAgngAFtioBFFh5AhSIJ0CBeAIUiCdAgXgCFLjbDlBg5QlQIJ4ABeIJUCCeAAXiCVAgngAF\ntioBFHRh5Tk9wmv2JTmd5MMkT092HIDtDQaDkR6TtN3K844kzyb5SZILSf6c5LdJTk10KoAb6MPK\n88EkZ5KcTfJ5kpeSPDzhmQBuqAsrz+3ieU+Stauef7zxOYDWdCGe2122j/TVV1dXNz+enZ3N3Nzc\nODMBO8TKykpWVlYaP24XLtu3i+eFJDNXPZ/JcPX5JQsLC03OBOwQi4uLWVxc3Hx+6NChRo7bha1K\n2122v53k3iTzSXYneTTDG0YArenDZfuVJE8keT3DO+/PxZ12oGV9uGxPktc2HgCd0Jd4AnSKeAIU\niCdAgXgCFHRhq5J4Ar1j5QlQIJ4ABeIJUCCeAAXiCVAgngAFtioBFFh5AhSIJ0CBeAIUiCdAgXgC\nFLjbDlBg5QlQIJ4ABeIJUCCeAAXiCVAgngAFtioBFFh5AhSIJ0CBeAIUiCfcJqamptoeYUfpQjyb\n+BUdJO3/RIA+mNr8ZgyDF198caQXPvbYY018vWtqZOW5tHSgicP03vLyYedig3OxZXn5cCb0/+9t\ny1YlgIIuXLaLJ9A74glQIJ4ABeIJUCCeAAXiCVBgqxJAgZUnQIF4AhSIJ0CBeAIUiCdAgXgCFNiq\nBFBg5QlQIJ4ABeIJUCCeAAXiCVDgbjtAgZUnQIF4AhSIJ0CBeAIUiCdAgXgCFHRhq9L0CK95PsnF\nJCcmPAvASAaDwUiP69iX5HSSD5M8XZ1hlHj+euOLAXTCGPG8I8mzGTbtu0n2J7m/MsMo8VxNcqly\ncIBJGCOeDyY5k+Rsks+TvJTk4coMo8QToFPGiOc9Sdauev7xxuduWiM3jFZXVzc/np2dzdzcXBOH\nBbim672f+dFHH+Xs2bM3/KFNzdBIPBcWFpo4DMBIrhfP+fn5zM/Pbz4/duzYV19yIcnMVc9nMlx9\n3jRblYDeGWOr0ttJ7k0yn+STJI9meNPopo3ynueRJG8muS/D9woer3whgKaM8Z7nlSRPJHk9yckk\nv0lyqjLDKCvPUpUBJmXMv2H02sZjLC7bgd7x1zMBCsQToEA8AQrEE6CgC/+qkngCvWPlCVAgngAF\n4glQIJ4ABeIJUCCeAAW2KgEUWHkCFIgnQIF4AhSIJ0CBeAIUuNsOUGDlCVAgngAF4glQIJ4ABeIJ\nUCCeAAW2KgEUWHkCFIgnQIF4AhR0IZ5TDRxjkLT/EwH6YGrzmzEM9u/fP9ILjxw50sTXu6ZGVp5L\nSweaOEzvLS8fdi42OBdbnIsty8vNHKcLK0+X7UDv2KoEUGDlCVAgngAF4glQIJ4ABeIJUCCeAAW2\nKgEUWHkCFIgnQIF4AhSIJ0CBeAIUiCdAga1KAAVWngAF4glQIJ4ABeIJUCCeAAXutgMUWHkCFIgn\nQIF4AhSIJ0CBeAIUdCGe0yO8ZibJ0SQfJHk/yZMTnQhgG+vr6yM9JmmUlefnSX6Z5N0ke5L8Jckb\nSU5NcC6A6+rCynOUeH668UiSyxlG8+6IJ9CSvsTzavNJHkjyVvOjAIymb/Hck+TlJE9luALdtLq6\nuvnx7Oxs5ubmGhkO6Ldz587l/PnzjR+3T/G8M8krSV5I8upX/+PCwkKTMwE7xNzc3JcWU8ePH2/k\nuH2J51SS55KcTPLMZMcB2F5f4vnjJL9I8l6SdzY+t5TkD5MaCuBG+vKvKh3PaPtBAW6Jvqw8ATpF\nPAEKxBOgQDwBCsQToEA8AQr6slUJoFOsPAEKxBOgQDwBCsQToEA8AQrEE6DAViWAAitPgALxBCgQ\nT4AC8QQoEE+AAvEEKLBVCaDAyhOgQDwBCsQToEA8AQq6EM/ptgdoyrlz59oeoTOciy3OxZaddC7W\n19dHekzSjonn+fPn2x6hM5yLLc7Flp10LgaDwUiPSXLZDvROFy7bxRPonS7Ec6qBY6wk2dvAcYCd\n71iSxTGPMdizZ89IL7x8+XLSTOf+TxMrz8UGjgEwsi6sPF22A70jngAFXfiHQXbCVqV9SU4n+TDJ\n0y3P0qbnk1xMcqLtQTpgJsnRJB8keT/Jk+2O06qvJXkrybtJTiZZbnecZnRhq1Lf3ZHkTJL5JHdm\n+Bvk/jYHatFCkgcinknynSTf3/h4T5K/5fb9fZEk39j4fleSPyV5qMVZmjDYvXv3SI8kEyto31ee\nD2YYz7NJPk/yUpKH2xyoRatJLrU9REd8muEfpElyOcmpJHe3N07r/rXx/e4MFxx/b3GWRkxo5fnz\nDK9W/pvkB9u9uO/xvCfJ2lXPP974HHxhPsMV+Vstz9Gm6Qz/MLmY4dsZJ9sdZ3wTiueJJI8k+eMo\nL+77DSNvanAje5K8nOSpDFegt6v1DN/G+FaS1zPcXrjS4jxjm9D7madv5sV9X3leyPDmwBdmMlx9\nwp1JXknyQpJXW56lK/6Z5PdJftT2IOPqwg2jvq88305yb4aXZp8keTTJ/jYHohOmkjyX4eXpMy3P\n0rZvJ7mS5B9Jvp7kp0kOtTpRA8bYqvRGhjcUv+pAkt+VB+qpn2V4N/VMkqWWZ2nTkQz/APlPhu8D\nP97uOK16KMNL1XeTvLPx2NfqRO35XpK/Zngu3kvyq3bHacTgJh6fFY5/NCPcMALgy44m+WHbQwD0\nxSMZXrX9O8Ptbq+1Ow4AAAAAAAAAAAAAANwi/wMdiqYLkC5QywAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f35cbfac090>"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Value Iteration\n",
      "---\n",
      "***\n",
      "* There are iterative methods that can get as the value\n",
      "* One of them is termed value iteration\n",
      "    * Start with some random V_0(s)\n",
      "    * Proceed with calculaint \n",
      "        * $ {V_{k+1} (s) =  R(s) + \\gamma\\max\\limits_{a \\in A}\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V_k(s')     \\right)}$\n",
      "        * Use only the next s'\n",
      "    * Repeat\n",
      "* Such methods are called \"Dynamic Programming\"\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "(Modified) Policy Iteration\n",
      "---\n",
      "***\n",
      "* Policy Iteration is an iterative, model-based method for attacking any kind of MDP\n",
      "* It does not make much sense in the case of finite MDPS without loops, use backups\n",
      "* Loop until policy no further changes\n",
      "    * Policy Evaluation: Find V-Values given a policy - start with a random policy\n",
      "        * Loop over states for a fixed number of iterations $k$\n",
      "        * For all states $s$\n",
      "            * $ {V^{\\pi_{n}} (s) =  R(s) + \\gamma\\sum\\limits_{a \\in A}\\pi_n(s,a)\\left( \\sum\\limits_{s' \\in S} T(s'|s,a) V^{\\pi_{n-1}}(s')     \\right)}$ \n",
      "        * Don't recurse until the end - just do it for the immidate next state!\n",
      "    * Policy Improvement: From new policy by acting greedily on Q-Values (infered from V-Values)\n",
      "        * $\\pi_{n+1}(s,a) = \\begin{cases} 1, & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  \\sum\\limits_{s' \\in S} T(s'|s,a) V^{\\pi_{n}}(s')$} \\\\ 0, & \\mbox{ otherwise} \\end{cases}$\n",
      "    \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class PIAgent():\n",
      "    \"\"\" A policy iteration agent\n",
      "    \"\"\"\n",
      "    def __init__(self,mdp, k):\n",
      "        self.mdp = mdp\n",
      "        self.k = k\n",
      "    \n",
      "    \n",
      "    def policyIteration(self):\n",
      "        \"\"\" Solve an MDP by policy iteration [Fig. 17.7, AIM], \n",
      "            A\n",
      "        \"\"\"\n",
      "        mdp = self.mdp\n",
      "        k = self.k\n",
      "        V = dict([(s, 0) for s in mdp.states])\n",
      "        pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
      "        while True:\n",
      "            V1 = V.copy()\n",
      "            V1 = self.policyEvaluation(pi, V1, mdp, k)\n",
      "            unchanged = True\n",
      "            for s in mdp.states:   \n",
      "                a = argmax(mdp.actions(s), lambda a: Q_value(a,s,V1,mdp))\n",
      "                if a != pi[s]:\n",
      "                    pi[s] = a\n",
      "                    unchanged = False\n",
      "            if unchanged:\n",
      "                self.V = V1\n",
      "                self.pi = pi\n",
      "                return\n",
      "\n",
      "    def policyEvaluation(self, pi, V, mdp,k ):\n",
      "        \"\"\"Return V-Values, using an approximation (modified policy iteration).\"\"\"\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        for i in range(k):\n",
      "            for s in mdp.states:\n",
      "                V[s] = R(s) + gamma * sum([p * V[s1] for (p, s1) in T(s, pi[s])])\n",
      "        return V\n",
      "    \n",
      "    def valueIteration(epsilon=0.001):\n",
      "        \"Solving an MDP by value iteration.\"\n",
      "        V1 = dict([(s, 0) for s in mdp.states])\n",
      "        R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "        while True:\n",
      "            V = V1.copy() \n",
      "            delta = 0\n",
      "            for s in mdp.states:\n",
      "                V1[s] = R(s) + gamma * max([sum([p * V[s1] for (p, s1) in T(s, a)])\n",
      "                                            for a in mdp.actions(s)])\n",
      "                delta = max(delta, abs(V1[s] - V[s]))\n",
      "            if delta < epsilon * (1 - gamma) / gamma:\n",
      "                self.V = V \n",
      "                return \n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gmdp = GridMDP(grid,terminals=terminals)\n",
      "pia = PIAgent(gmdp, 50)\n",
      "pia.policyIteration()\n",
      "values, policy  = pia.V, pia.pi\n",
      "vsl = Visualiser()\n",
      "plt = vsl.to_plt_arrows(grid, values, policy, fig = \"grid_solved.png\" )\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD3CAYAAACdI9ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI0mSJEmSJEmSJEmSJEmSJElSP/k/Kx1shyi1yzQAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f35cbdcdfd0>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Discovered Policy and Values\n",
      "----\n",
      "***\n",
      "States/Rewards       | Actions  \n",
      "-------------        | --------\n",
      "<img src=\"grid_solved.png\"> | left, right, up, down\n",
      "\n",
      "<p style='text-align: center;'>**Transitions**</p>\n",
      "<p style='text-align: center;'>$T(s\\_action|\\cdot, s\\_action) = 0.8 \\\\ T(s\\_left|\\cdot,s\\_action) = 0.1\\\\ T(s\\_right|\\cdot,s\\_action) = 0.1 $</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Free Methods\n",
      "---\n",
      "***\n",
      "* We have assumed the agent has a copy of the MDP (a *model*) in its head\n",
      "* This is rarely the case\n",
      "* Generaly the agent can only interact with the envinroment\n",
      "* i.e. the agent is imbued in a statistical bath of states, actions, trantisions and rewards\n",
      "* How can the agent act then ? \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (1)\n",
      "---\n",
      "***\n",
      "* Remember Q is just E[\n",
      "* MC (Naive Version)\n",
      "    * Start at any state, initialise $Q_0(s,a)$ as you visit states/actions \n",
      "    * Act $\\epsilon$-greedily\n",
      "        * $\\pi^\\epsilon(s,a) = \\begin{cases} 1-\\epsilon + \\epsilon/|A| , & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q_k(s,a)$} \\\\ \\epsilon/|A|, & \\mbox{if } \\mbox{ otherwise} \\end{cases}$\n",
      "    * Wait until episode ends\n",
      "    * Add all reward you have seen so far to $\\mathrm{v_\\tau^i}  = R(s')+ \\gamma R(s'') + \\gamma^2 R(s''') + \\gamma^{\\tau-1}R(s^\\tau)$ for episode $i$\n",
      "    * $Q_k(s,a) =  E_{\\pi^\\epsilon}[\\mathrm{v_\\tau^i}]  = \\frac{1}{k}\\sum\\limits_{i=1}^{n}{\\mathrm{v_\\tau^i}} $, where $k$ is the times a state is visited  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (2)\n",
      "---\n",
      "***\n",
      "* $\\epsilon$-greedy means acting greedily $1-\\epsilon$, random otherwise\n",
      "* Better to calculate mean incrementaly\n",
      "\\begin{align*}\n",
      "Q_k(s,a) &= E_{\\pi_k}[\\mathrm{v_\\tau^i}]\\\\\n",
      "Q_k(s,a) &= \\frac{1}{k}\\sum\\limits_{i=1}^{k}{\\mathrm{v_\\tau^i}}\\\\\n",
      "Q_k(s,a) &= \\frac{1}{k}\\left(\\mathrm{v_t^1} + \\mathrm{v_\\tau^2}....\\mathrm{v_\\tau^{k-1}} +  \\mathrm{v_\\tau^k}\\right)\\\\\n",
      "Q_k(s,a) &= \\frac{1}{k}\\left(\\sum\\limits_{i=1}^{ n-1}{\\mathrm{v_\\tau^i}} +  \\mathrm{v_\\tau^k}\\right)\\\\\n",
      "\\end{align*}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control (3)\n",
      "---\n",
      "***\n",
      "by definition $Q_{k-1}(s,a) = \\frac{1}{{k-1}}\\sum\\limits_{i=1}^{k-1}{\\mathrm{v_\\tau^i}} \\implies (k-1)Q_{k-1}(s,a) =  \\sum\\limits_{i=1}^{k-1}{\\mathrm{v_\\tau^i}} $\n",
      "\\begin{align*}\n",
      "Q_k(s,a) &= \\frac{1}{k}\\left((k-1)Q_{k-1}(s,a) +  \\mathrm{v_\\tau^k}\\right) \\\\\n",
      "Q_k(s,a) &= \\frac{1}{k}\\left(Q_{k-1}(s,a)k - Q_{k-1}(s,a) + \\mathrm{v_\\tau^k}\\right)\\\\\n",
      "Q_k(s,a) &= \\frac{Q_{k-1}(s,a)k}{k} + \\frac{-Q_{k-1}(s,a) + \\mathrm{v_\\tau^k}}{n}\\\\\n",
      "Q_k(s,a) &= Q_{k-1}(s,a) + \\frac{\\overbrace{\\mathrm{v_\\tau^k} - Q_{k-1}(s,a)}^{\\textbf{MC-Error}} }{k}\\\\\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Monte Carlo Control - Putting it all togeather\n",
      "---\n",
      "***\n",
      "* But $pi^n$ changes continously, so the expectation is non-stationary\n",
      "\\begin{align*}\n",
      "Q_k(s,a) &= Q_{k-1}(s,a) + \\frac{1}{k}\\left[\\mathrm{v_\\tau^k} - Q_{k-1}(s,a) \\right] \\rightarrow \\textbf{bandit case} \\\\\n",
      "Q_k(s,a) &= Q_{k-1}(s,a) + \\alpha\\left[\\mathrm{v_\\tau^k} - Q_{k-1}(s,a) \\right]  \\rightarrow \\textbf{Full MDP case}\\\\\n",
      "\\end{align*}\n",
      "\n",
      "* $\\alpha$ is a learning rate, usually set to a small value (e.g., 0.001)\n",
      "\n",
      "* MC\n",
      "    * Start at any state, initialise $Q_0(s,a)$ as you visit states/actions \n",
      "    * Act $\\epsilon$-greedily\n",
      "        * $\\pi^\\epsilon(s,a) = \\begin{cases} 1-\\epsilon + \\epsilon/|A| , & \\mbox{if } \\mbox{ $a = \\underset{a \\in A}{\\operatorname{argmax}}  Q_k(s,a)$} \\\\ \\epsilon/|A|, & \\mbox{if } \\mbox{ otherwise} \\end{cases}$\n",
      "    * Wait until episode ends\n",
      "    * Add all reward you have seen so far to $\\mathrm{v_\\tau^i}  = R(s)+\\gamma R(s')+...\\gamma^2R(s'') + \\gamma^{\\tau-1}R(s^\\tau)$ for episode $i$\n",
      "    * $Q_k(s,a) = Q_{k-1}(s,a) + \\alpha\\left[\\mathrm{v_\\tau^k} - Q_{k-1}(s,a) \\right]$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simplified BlackJackWorld\n",
      "---\n",
      "***\n",
      "* We are going to use a simplified version of BlacJack, a popular card game, as an example\n",
      "* Though it is possible to create a model, it's not that trivial, which makes it ideal for model-free methods\n",
      "* Game Rules as follows\n",
      "    * All cards have the face values, except J,Q,K that have a value of 10 and A that has a value of 11\n",
      "    * Goal of the game is to reach 21 - or if you have two aces 22 (that's a deviation from the original game)\n",
      "    * A player and a Dealer are dealt 2 cards each - the dealer keeps the second card hidden\n",
      "    * A player can \"stick\" - no more cards received or \"hit\" (get more cards)\n",
      "        * If the score of 21 (or 22 in case of two aces) is passed, the player loses (reward -1)\n",
      "        * If the player has a higher score than the dealer, he wins (reward +1)\n",
      "        * If scores equal (reward 0)\n",
      "    * Dealer has a fixed policy, if score less than 17, she has to draw more cards"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from blackjack import SimpleBlackJack\n",
      "import utils\n",
      "\n",
      "\n",
      "class RLAgent():\n",
      "    \"\"\"A SARSA/MC Agent\n",
      "    \"\"\"\n",
      "    def __init__(self,learning_rate = 0.01, epsilon = 0.01):\n",
      "        self.Q = {}\n",
      "        self.learning_rate = learning_rate\n",
      "        self.epsilon = epsilon\n",
      "\n",
      "    def MC(self,visited_states, actions, rewards):\n",
      "        \"\"\"\" Monte Carlo Control \"\"\"\n",
      "        v_t = sum(rewards)\n",
      "\n",
      "        for i in range(len(visited_states)):\n",
      "            state = visited_states[i]\n",
      "            action = actions[i]\n",
      "            #if(state[0] == 4 and action == \"stick\"):\n",
      "            #    print state, action, visited_states,actions, rewards\n",
      "            state_action, Q = self.__Q(state, action)\n",
      "            self.Q[state_action]+= self.learning_rate*(v_t - self.Q[state_action] )\n",
      "\n",
      "    def SARSA(self, previous_state, previous_action, state, action, next_reward):\n",
      "        \"\"\"\" SARSA(0) \"\"\"\n",
      "        if(state is None):\n",
      "            Q = 0\n",
      "        else:\n",
      "            state_action, Q = self.__Q(state, action)\n",
      "        previous_state_action, previous_Q = self.__Q(previous_state, previous_action)\n",
      "        #self.Q[state_action]+= self.learning_rate*(reward + previous_Q - self.Q[state_action] )\n",
      "        self.Q[previous_state_action]+= self.learning_rate*(next_reward + Q - self.Q[previous_state_action]  )\n",
      "\n",
      "\n",
      "    def selectAction(self, state, actions):\n",
      "        \"\"\" Select an action given a state and a vector of possible actions\"\"\"\n",
      "        r = random.random()\n",
      "        if(len(actions) == 1):\n",
      "            return actions[0]\n",
      "        if(r < self.epsilon):\n",
      "            return random.choice(actions)\n",
      "        else:\n",
      "\n",
      "            return utils.argmax(actions, lambda action: self.__Q(state,action)[1])\n",
      "\n",
      "    def __Q(self,state, action):\n",
      "        if(state is None or action is None):\n",
      "            return 0\n",
      "        state_action = tuple(state + [action])\n",
      "\n",
      "        if(state_action not in self.Q):\n",
      "            self.Q[state_action] = 0\n",
      "\n",
      "        return state_action, self.Q[state_action]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blackjack = SimpleBlackJack()\n",
      "agent = RLAgent(learning_rate=0.01, epsilon=0.01)\n",
      "blackjack.simulate(agent,200000,episodic=True)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vis = Visualiser()\n",
      "vis.Q(agent.Q, \"RL-MC.png\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAADyCAYAAADnXrZCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f9b2e4a04d0>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Blackjack MC Results\n",
      "---\n",
      "***\n",
      "\n",
      "<img src=\"RL-MC.png\">\n",
      "* 16 Onwards \"stick\", less than that \"hit\"\n",
      "* Is this correct - looks like it is\n",
      "* Can you think of an adequete test ? \n",
      "* What if there are not terminal states ? \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "SARSA(0)\n",
      "---\n",
      "***\n",
      "* Stands for State-Action-Reward-State-Action\n",
      "* 0 stands for a paramterer called $\\lambda$, which we are going to ignore here\n",
      "* The MC update rule \n",
      "    * $Q_k(s,a) = Q_{k-1}(s,a) + \\alpha\\left[\\mathrm{v_\\tau^k} - Q_{k-1}(s,a) \\right]$\n",
      "* Let's try to create an incremental version of MC\n",
      "\n",
      "\\begin{align*}\n",
      "\\mathrm{v_\\tau^i}  &= R(s')+ \\gamma R(s'') + \\gamma^2 R(s''') + \\gamma^{\\tau-1}R(s^\\tau) \\\\\n",
      "\\mathrm{v_\\tau^k} &=  R(s') + \\gamma \\left(R(s'') + \\gamma R(s''') + \\gamma^{\\tau-1}R(s^\\tau)\\right)  \\\\\n",
      "\\mathrm{v_\\tau^k} &=  R(s') + \\gamma \\mathrm{v_\\tau^{k '}} \\\\\n",
      "E_{\\pi_k}[\\mathrm{v_\\tau^i}] &= E_{\\pi_k}[R(s') + \\gamma \\mathrm{v_\\tau^{k '}}]\\\\\n",
      "E_{\\pi_k}[\\mathrm{v_\\tau^i}] &= E_{\\pi_k}[R(s')] + E_{\\pi_k}[\\gamma \\mathrm{v_\\tau^{k '}}]\\\\\n",
      "E_{\\pi_k}[\\mathrm{v_\\tau^i}] &= E_{\\pi_k}[R(s') + \\gamma Q(s',a')]\\\\\n",
      "\\mathrm{v_\\tau^k} &=  R(s') + \\gamma Q(s',a') \n",
      "\\end{align*}\n",
      "\n",
      "    \n",
      "* $Q_k(s,a) = Q_{k-1}(s,a) + \\alpha\\left[R(s') + \\gamma Q_{k-1}(s',a') - Q_{k-1}(s,a) \\right]$\n",
      "    \n",
      "* Dynamic Programming targets are closer to SARSA rathen than MC\n",
      "    \n",
      "    \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blackjack = SimpleBlackJack()\n",
      "agent = RLAgent(learning_rate=0.01, epsilon=0.01)\n",
      "blackjack.simulate(agent,200000,episodic=False)\n",
      "\n",
      "from visualisation import Visualiser\n",
      "vis = Visualiser()\n",
      "vis.Q(agent.Q, \"RL-SARSA.png\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'SimpleBlackJack' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-24-cc4e33a627dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mblackjack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleBlackJack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRLAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mblackjack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisodic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvisualisation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVisualiser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'SimpleBlackJack' is not defined"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Blackjack SARSA Results\n",
      "---\n",
      "***\n",
      "MC       | SARSA  \n",
      "-------------        | --------\n",
      "<img src=\"RL-MC.png\"> | <img src=\"RL-SARSA.png\"> \n",
      "\n",
      "\n",
      "* 16 Onwards \"stick\", less than that \"hit\"\n",
      "* Similiar to MC\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MC vs SARSA\n",
      "---\n",
      "***\n",
      "* But not exactly the same\n",
      "* SARSA(0) has lower variance\n",
      "* Requires stronger Markov Guarrantees\n",
      "* MC works even with environments with sensor aliasing/partially observability\n",
      "* MC slower to converge, higher variance\n",
      "* In very long chains, MC should also be prefered, especially if rewards are very delayed, as initial states may not see any rewards fast enough"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "References\n",
      "---\n",
      "***\n",
      "* RL is a massive topic\n",
      "* We have shown the tip of iceberg\n",
      "* Rabbit hole goes deep - there are numerous applications\n",
      "* Department has a PhD programme associated with RL \n",
      "    * ICGI Centre\n",
      "    * Four year long PhD programme\n",
      "    \n",
      "    \n",
      "        \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "References\n",
      "---\n",
      "***\n",
      "* David Silver's UCL Course: http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html\n",
      "    * Some ideas in this lecture notes taken from there\n",
      "    * Probably the best set of notes there is on the subject\n",
      "    * Lectures 1,2,3,4 \n",
      "* Reinforcement Learning, by Richard S. Sutton and Andrew G. Barto \n",
      "    * Classic book\n",
      "    * Excellent treatment of most subjects\n",
      "    * Up to Chapter 5\n",
      "* [AIM],Artificial Intelligence: A Modern Approach by Stuart J. Russell and Peter Norvig\n",
      "    * The Introductory A.I. Textbook\n",
      "    * Chapters 16 and 21\n",
      "    * Examples used here from this book, but different terminology\n",
      "* Algorithms for Reinfocement Learning by Csaba Szepesvari\n",
      "    * Very \"Mathematical\", but a good resource that provides a very unified view of the field\n",
      "* Reinforcement Learning: State-Of-The-Art by Marco Wiering (Editor), Martijn Van Otterlo (Editor) \n",
      "    * Edited Volume\n",
      "    * Chapter 1\n",
      "        \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Part Two Review Questions\n",
      "---\n",
      "***\n",
      "* How is a Q-Value related to the mean rewards ?\n",
      "* What is relationship between V and Q Values ?\n",
      "* Do value iteration for the first 3 steps in EagleWorld MDP\n",
      "* Perform the first 3 steps of policy iteration in EagleWorld MDP\n",
      "* Run two iterations of SARSA in EagleWorld MDP\n",
      "* Run two iterations of MC in EagleWorld MDP\n",
      "* You run MC in uknown enviroment and you find out that each chain as 50 steps, all containing different states. You have a very limited budget of iterations $k=30$. Would you switch to SARSA why ?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}